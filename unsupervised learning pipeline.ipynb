{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe each location with companies in side\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pygeohash as pgh\n",
    "from math import *\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "pjoin = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function_base\n",
    "def splitdat(dat,key_column=['duns_number'],right_colunm='atlas_location_uuid_tr',rate_tr=0.8):\n",
    "    \"\"\"\n",
    "    split the <company,location> pair into training/testing dat\n",
    "    \"\"\"\n",
    "    tr = dat.sample(frac=rate_tr)\n",
    "    tt = pd.merge(dat,tr,on=key_column,how='left',suffixes=['','_tr'])\n",
    "    tt = tt[tt[right_colunm].isnull()]\n",
    "    tt = tt[list(tr.columns)]\n",
    "    print('Train dat:', len(tr), 'Test dat:', len(tt))\n",
    "    return tr,tt\n",
    "\n",
    "#data process\n",
    "def onehotdat(dat,key_column:list):\n",
    "    dat[key_column] = dat[key_column].astype(str)\n",
    "    dum_dat = pd.get_dummies(dat[key_column],dummy_na=True)\n",
    "    return dum_dat\n",
    "\n",
    "def split2num(emp_range:str):\n",
    "    max_emp_val = emp_range.replace(' ','').split('-')\n",
    "    if len(max_emp_val)<2:\n",
    "        return 10\n",
    "    else:\n",
    "        return float(max_emp_val[1])\n",
    "    \n",
    "def max_col(dat,col,minval=1):\n",
    "    dat[col] = dat[col].apply(lambda r:max(r,minval))\n",
    "\n",
    "def comp_dat_process(dat):\n",
    "    \"\"\"\n",
    "    pd -> company key,cont_feature,spec_feature,dum_feature\n",
    "    \"\"\"\n",
    "    one_hot_col_name = ['major_industry_category','location_type','primary_sic_2_digit']\n",
    "    spec_col_name = 'emp_here_range'\n",
    "    cont_col_name = ['emp_here','emp_total','sales_volume_us','square_footage']\n",
    "\n",
    "    print('doing one-hot...')\n",
    "    dum_dat = onehotdat(dat,one_hot_col_name)\n",
    "    \n",
    "    print('extract continuous...')\n",
    "    cont_dat = dat[cont_col_name].fillna(value=0).astype(float)\n",
    "    \n",
    "    print('specific feature')\n",
    "    spec_dat = dat[spec_col_name].fillna(value='1-10').astype(str)\n",
    "    spec_dat = spec_dat.apply(lambda row: split2num(row))\n",
    "    \n",
    "    max_col(cont_dat,'emp_here',1)\n",
    "    \n",
    "    res_dat = dat[['duns_number']].join([cont_dat,spec_dat,dum_dat],how='left')\n",
    "    assert(len(res_dat)==len(dum_dat))\n",
    "    assert(len(res_dat)==len(cont_dat))\n",
    "    assert(len(res_dat)==len(spec_dat))\n",
    "    return res_dat\n",
    "\n",
    "def comp_transpd2np(featdat,trdat,ttdat,not_col_name):\n",
    "    tr_feat = pd.merge(trdat,featdat,on='duns_number',how='inner')\n",
    "#     print(col_list)\n",
    "    col_list = [ n for n in list(tr_feat.columns) if n not in not_col_name ] \n",
    "    trainX = tr_feat.loc[:,col_list].to_numpy()\n",
    "    trainY = tr_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    \n",
    "    tt_feat = pd.merge(ttdat,featdat,on='duns_number',how='inner')\n",
    "    col_list = [ n for n in list(tt_feat.columns) if n not in not_col_name ] \n",
    "#     print(col_list)\n",
    "    testX = tt_feat.loc[:,col_list].to_numpy()\n",
    "    testY = tt_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    return trainX,trainY,testX,testY\n",
    "\n",
    "def normalize_dat(trX,ttX,cols=5,axis=0):\n",
    "    D = trX[:,:cols]\n",
    "    center = D.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = D.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    trX[:,:cols] = (D-center)/scale\n",
    "    ttX[:,:cols] = (ttX[:,:cols]-center)/scale\n",
    "    \n",
    "def calc_topk_acc_v2(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    \"\"\"\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    match_array = np.logical_or.reduce(max_k_cat==y_truth_cat, axis=1) #得到匹配结果\n",
    "    topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "    return topk_acc_score\n",
    "\n",
    "def calc_topk_acc_cat_all(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    return top1-topk acc\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    M = max_k_cat==y_truth_cat\n",
    "    for k in range(M.shape[1]):\n",
    "        match_array = np.logical_or.reduce(M[:,:k+1], axis=1) #得到匹配结果\n",
    "        topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "        res.append(topk_acc_score)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load\n",
    "datapath = '/Users/yefeichen/Database/location_recommender_system/'\n",
    "cfile = ['dnb_pa.csv','dnb_sf.csv','dnb_sj.csv']\n",
    "lfile = 'location_scorecard_190912.csv'\n",
    "clfile = ['PA.csv','SF.csv','SJ.csv']\n",
    "\n",
    "ind_city = 2\n",
    "\n",
    "pdc = pd.read_csv(pjoin(datapath,cfile[ind_city]))\n",
    "pdl = pd.read_csv(pjoin(datapath,lfile))\n",
    "pdcl = pd.read_csv(pjoin(datapath,clfile[ind_city]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dat: 20032 Test dat: 5008\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "48377\n",
      "(5008, 20032)\n",
      "Train dat: 20032 Test dat: 5008\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "48377\n",
      "(5008, 20032)\n",
      "Train dat: 20032 Test dat: 5008\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "48377\n",
      "(5008, 20032)\n",
      "Train dat: 20032 Test dat: 5008\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "48377\n",
      "(5008, 20032)\n",
      "Train dat: 20032 Test dat: 5008\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "48377\n",
      "(5008, 20032)\n"
     ]
    }
   ],
   "source": [
    "#nfolder test \n",
    "not_col_name = ['duns_number','atlas_location_uuid','geo_distance','longitude_loc','latitude_loc']\n",
    "nfold = 10\n",
    "K = 50\n",
    "topk_acc = []\n",
    "for n in range(nfold):\n",
    "    tr,tt = splitdat(pdcl)\n",
    "    tr['duns_number']=tr['duns_number'].astype(int)\n",
    "    proc_pdc = comp_dat_process(pdc)\n",
    "    print(len(proc_pdc))\n",
    "    # proc_pdc.head()\n",
    "    trX,trY,ttX,ttY = comp_transpd2np(proc_pdc,tr,tt,not_col_name=not_col_name)\n",
    "    normalize_dat(trX,ttX,cols=5)\n",
    "    distQR = euclidean_distances(ttX,trX)\n",
    "    print(distQR.shape)\n",
    "    \n",
    "    topk_acc.append(calc_topk_acc_cat_all(-distQR,ttY[:,0],trY[:,0],k=K))\n",
    "    \n",
    "#plot    \n",
    "R = np.array(topk_acc)\n",
    "mR = R.mean(axis=0)\n",
    "stdR = R.std(axis=0)\n",
    "\n",
    "x = list(range(1,51))\n",
    "y = list(mR)\n",
    "stdy = list(stdR)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.errorbar(x,y,yerr=stdy,fmt='^',mfc='yellow',mec='blue',ms=5,mew=2)\n",
    "\n",
    "plt.xlabel(\"topk\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"topk accuracy curve of %s\"%cfile[ind_city])\n",
    "\n",
    "plt.savefig('1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n"
     ]
    }
   ],
   "source": [
    "#generate cross feature of company and location for building\n",
    "def comp_transpd2np_single(featdat,trdat):\n",
    "    tr_feat = pd.merge(trdat,featdat,on='duns_number',how='inner')\n",
    "    col_list = list(tr_feat.columns)\n",
    "    X = tr_feat.loc[:,col_list[3:]].to_numpy()\n",
    "    Y_comp = tr_feat[['duns_number']].to_numpy()\n",
    "    Y_loc = tr_feat[['atlas_location_uuid']].to_numpy()\n",
    "    \n",
    "    return X,Y_comp,Y_loc\n",
    "\n",
    "proc_pdc = comp_dat_process(pdc)\n",
    "X,Y_comp,Y_loc = comp_transpd2np_single(proc_pdc,pdcl)\n",
    "distQR = euclidean_distances(X,X)\n",
    "Y_comp_Y = np.tile(Y_comp,(1,Y_comp.shape[0])).reshape(-1,1)\n",
    "Y_loc_Y = np.tile(Y_loc,(1,Y_loc.shape[0])).transpose().reshape(-1,1)\n",
    "distQR = distQR.reshape(-1,1)\n",
    "\n",
    "M = np.concatenate([Y_comp_Y,Y_loc_Y,distQR],axis=1)\n",
    "pdM = pd.DataFrame(data=M,columns=['duns_number','atlas_location_uuid','sim_score'])\n",
    "pdM['sim_score_new'] = pdM.sim_score.apply(lambda r: 1e6 if r<0.05 else r)\n",
    "crossFeatM = pdM.groupby(['duns_number','atlas_location_uuid'])[['sim_score_new']].min()\n",
    "crossFeatM.to_csv('SJ_comp_loc_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
