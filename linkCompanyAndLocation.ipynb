{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pygeohash as pgh\n",
    "from math import *\n",
    "pjoin = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load\n",
    "datapath = '/Users/yefeichen/Database/location_recommender_system/'\n",
    "cfile = ['dnb_pa.csv','dnb_sf.csv','dnb_sj.csv']\n",
    "lfile = 'location_scorecard_190912.csv'\n",
    "\n",
    "pdc1 = pd.read_csv(pjoin(datapath,cfile[0]))\n",
    "pdc2 = pd.read_csv(pjoin(datapath,cfile[1]))\n",
    "pdc3 = pd.read_csv(pjoin(datapath,cfile[2]))\n",
    "\n",
    "# pdc = pd.concat([pdc1,pdc2,pdc3],axis=0)\n",
    "pdl = pd.read_csv(pjoin(datapath,lfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#city filter\n",
    "def cityfilter(datComp,datLoc):\n",
    "    city = datComp.groupby(['physical_city'],as_index=False)['physical_city'].agg({'cnt':'count'})\n",
    "    print(len(city))\n",
    "    pdatLoc = pd.merge(datLoc,city,how='inner',left_on = ['city'],right_on=['physical_city'],suffixes=['_loc','_comp'])\n",
    "    return pdatLoc\n",
    "\n",
    "def geohash(data,precision=6):\n",
    "    data['geohash'] = data.apply(lambda row:pgh.encode(row['longitude'],row['latitude'],precision=precision),axis=1)\n",
    "\n",
    "\n",
    "def geo_distance(lng1,lat1,lng2,lat2):\n",
    "    lng1, lat1, lng2, lat2 = map(radians, [lng1, lat1, lng2, lat2])\n",
    "    dlon=lng2-lng1\n",
    "    dlat=lat2-lat1\n",
    "    a=sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2 \n",
    "    dis=2*asin(sqrt(a))*6371*1000\n",
    "    return dis\n",
    "\n",
    "def duplicateCheck(data,colname:str)->bool:\n",
    "    R = data.groupby([colname],as_index=False)[colname].agg({'cnt':'count'})\n",
    "    R = R[R['cnt']>1]\n",
    "    if len(R)>0:\n",
    "        print('duplicate detected')\n",
    "        return False\n",
    "    else:\n",
    "        print('not duplicate')\n",
    "        return True\n",
    "    \n",
    "def calcLinkTable(datComp,datLoc,verbose=True):\n",
    "    if not verbose:\n",
    "        print('merging...')\n",
    "    df_cartesian = pd.merge(datComp, datLoc,on = 'geohash', how='outer',suffixes=['_comp','_loc'])\n",
    "    if not verbose:\n",
    "        print(list(df_cartesian.columns))\n",
    "        print(len(df_cartesian))\n",
    "        print('calc geo dist...')\n",
    "    df_cartesian['geo_distance']=df_cartesian.apply(lambda row:geo_distance(row['longitude_comp'],row['latitude_comp'],row['longitude_loc'],row['latitude_loc']),axis=1)\n",
    "    if not verbose:\n",
    "        print('sort geo dist')\n",
    "    df_cartesian_min_distance=df_cartesian.sort_values(by=\"geo_distance\").groupby([\"duns_number\"],as_index=False).first()\n",
    "\n",
    "    result = df_cartesian_min_distance[['duns_number','atlas_location_uuid','geo_distance']]\n",
    "    if not verbose:\n",
    "        duplicateCheck(result , 'atlas_location_uuid')\n",
    "    return result\n",
    "\n",
    "def fuzzy_geosearch(datComp,datLoc,precision=[8,7,6,5],thresh=[500,1000,1000,1000]):\n",
    "    print('Initial company num:',len(datComp))\n",
    "    datLoc_city = cityfilter(datComp,datLoc)\n",
    "    print(len(datComp),len(datLoc_city))\n",
    "    datComp_city = datComp[['duns_number','longitude','latitude']]\n",
    "    datLoc_city = datLoc_city[['atlas_location_uuid','longitude','latitude']]\n",
    "    datlist = []\n",
    "    \n",
    "    for i,p in enumerate(precision):\n",
    "        print('level:',p)\n",
    "        geohash(datComp_city,p)\n",
    "        geohash(datLoc_city,p)\n",
    "        linkCL = calcLinkTable(datComp_city,datLoc_city)\n",
    "        datlist.append(linkCL[linkCL['geo_distance'] <= thresh[i]])\n",
    "        unmatched = linkCL[linkCL['geo_distance'] > thresh[i]].groupby('duns_number',as_index=False).first()\n",
    "        datComp_city = pd.merge(datComp_city,unmatched['duns_number'],on='duns_number',how='inner')\n",
    "        print('datComp_city:',len(datComp_city))\n",
    "        \n",
    "    res = pd.concat(datlist,axis=0,ignore_index=True)\n",
    "    print('Initial company num:',len(datComp), 'vs. Remain company num:', len(res) , 'rate:=', float(len(res))/len(datComp) )\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial company num: 7538\n",
      "1\n",
      "7538 442\n",
      "level: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datComp_city: 2672\n",
      "level: 7\n",
      "datComp_city: 1370\n",
      "level: 6\n",
      "datComp_city: 292\n",
      "level: 5\n",
      "datComp_city: 102\n",
      "Initial company num: 7538 vs. Remain company num: 6219 rate:= 0.8250198991775006\n",
      "Initial company num: 67849\n",
      "1\n",
      "67849 1845\n",
      "level: 8\n",
      "datComp_city: 22981\n",
      "level: 7\n",
      "datComp_city: 8036\n",
      "level: 6\n",
      "datComp_city: 2331\n",
      "level: 5\n",
      "datComp_city: 2014\n",
      "Initial company num: 67849 vs. Remain company num: 56490 rate:= 0.8325841206207902\n",
      "Initial company num: 48377\n",
      "1\n",
      "48377 670\n",
      "level: 8\n",
      "datComp_city: 20940\n",
      "level: 7\n",
      "datComp_city: 13443\n",
      "level: 6\n",
      "datComp_city: 5192\n",
      "level: 5\n",
      "datComp_city: 4199\n",
      "Initial company num: 48377 vs. Remain company num: 25040 rate:= 0.5176013394795047\n"
     ]
    }
   ],
   "source": [
    "linkCL1 = fuzzy_geosearch(pdc1,pdl)\n",
    "linkCL1.to_csv(pjoin(datapath,'PA.csv'),index = None, header=True)\n",
    "\n",
    "linkCL2 = fuzzy_geosearch(pdc2,pdl)\n",
    "linkCL2.to_csv(pjoin(datapath,'SF.csv'),index = None, header=True)\n",
    "\n",
    "linkCL3 = fuzzy_geosearch(pdc3,pdl)\n",
    "linkCL3.to_csv(pjoin(datapath,'SJ.csv'),index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
