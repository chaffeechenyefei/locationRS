{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe each location with companies in side\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pygeohash as pgh\n",
    "from math import *\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from matplotlib import pylab\n",
    "from sklearn.preprocessing import normalize\n",
    "pjoin = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function_base\n",
    "def getPosNegdat(dat):\n",
    "    \"\"\"\n",
    "    dat: pos pair of data (location,company,geo,distance)\n",
    "    return pos/neg pair of data, same structure of dat except one more column for label\n",
    "    \"\"\"\n",
    "    shuffle_dat = dat.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # shuffle_dat.head()\n",
    "\n",
    "    twin_dat = dat.join(shuffle_dat,how='left',lsuffix='_left',rsuffix='_right')\n",
    "    twin_dat = twin_dat[twin_dat['atlas_location_uuid_left'] != twin_dat['atlas_location_uuid_right']]\n",
    "    print(len(twin_dat))\n",
    "    twin_dat.head()\n",
    "\n",
    "    neg_datA = twin_dat[['duns_number_left','atlas_location_uuid_right','longitude_loc_right','latitude_loc_right']]\n",
    "    neg_datA = neg_datA.rename(columns={'duns_number_left':'duns_number','atlas_location_uuid_right':'atlas_location_uuid','longitude_loc_right':'longitude_loc','latitude_loc_right':'latitude_loc'})\n",
    "\n",
    "    neg_datB = twin_dat[['duns_number_right','atlas_location_uuid_left','longitude_loc_left','latitude_loc_left']]\n",
    "    neg_datB = neg_datB.rename(columns={'duns_number_right':'duns_number','atlas_location_uuid_left':'atlas_location_uuid','longitude_loc_left':'longitude_loc','latitude_loc_left':'latitude_loc'})\n",
    "\n",
    "    neg_dat = pd.concat([neg_datA,neg_datB],axis=0)\n",
    "    neg_dat['label'] = 0\n",
    "    dat['label'] = 1\n",
    "    res_dat = pd.concat([dat[['duns_number','atlas_location_uuid','longitude_loc','latitude_loc','label']],neg_dat],axis=0)\n",
    "    print('Neg dat num:',len(neg_dat),';Pos dat num:',len(dat))\n",
    "    return res_dat\n",
    "\n",
    "def splitdat(dat,key_column=['duns_number'],right_colunm='atlas_location_uuid_tr',rate_tr=0.8):\n",
    "    \"\"\"\n",
    "    split the <company,location> pair into training/testing dat\n",
    "    \"\"\"\n",
    "    tr = dat.sample(frac=rate_tr)\n",
    "    tt = pd.merge(dat,tr,on=key_column,how='left',suffixes=['','_tr'])\n",
    "    tt = tt[tt[right_colunm].isnull()]\n",
    "    tt = tt[list(tr.columns)]\n",
    "    print('Train dat:', len(tr), 'Test dat:', len(tt))\n",
    "    return tr,tt\n",
    "\n",
    "#data process\n",
    "def onehotdat(dat,key_column:list,dummy_na=True):\n",
    "    dat[key_column] = dat[key_column].astype(str)\n",
    "    dum_dat = pd.get_dummies(dat[key_column],dummy_na=dummy_na)#it has nan itself\n",
    "    return dum_dat\n",
    "\n",
    "def split2num(emp_range:str):\n",
    "    max_emp_val = emp_range.replace(' ','').split('-')\n",
    "    if len(max_emp_val)<2:\n",
    "        return 10\n",
    "    else:\n",
    "        return float(max_emp_val[1])\n",
    "    \n",
    "def max_col(dat,col,minval=1):\n",
    "    dat[col] = dat[col].apply(lambda r:max(r,minval))\n",
    "\n",
    "def comp_dat_process(dat):\n",
    "    \"\"\"\n",
    "    pd -> company key,cont_feature,spec_feature,dum_feature\n",
    "    \"\"\"\n",
    "    one_hot_col_name = ['major_industry_category','location_type','primary_sic_2_digit']\n",
    "    spec_col_name = 'emp_here_range'\n",
    "    cont_col_name = ['emp_here','emp_total','sales_volume_us','square_footage']\n",
    "\n",
    "    print('doing one-hot...')\n",
    "    dum_dat = onehotdat(dat,one_hot_col_name)\n",
    "    \n",
    "    print('extract continuous...')\n",
    "    cont_dat = dat[cont_col_name].fillna(value=0).astype(float)\n",
    "    \n",
    "    print('specific feature')\n",
    "    spec_dat = dat[spec_col_name].fillna(value='1-10').astype(str)\n",
    "    spec_dat = spec_dat.apply(lambda row: split2num(row))\n",
    "    \n",
    "    max_col(cont_dat,'emp_here',1)\n",
    "    \n",
    "    res_dat = dat[['duns_number']].join([cont_dat,spec_dat,dum_dat],how='left')\n",
    "    assert(len(res_dat)==len(dum_dat))\n",
    "    assert(len(res_dat)==len(cont_dat))\n",
    "    assert(len(res_dat)==len(spec_dat))\n",
    "    return res_dat\n",
    "\n",
    "def location_dat_process(dat):\n",
    "    \"\"\"\n",
    "    pd -> location key,cont_feature,dum_feature\n",
    "    \"\"\"\n",
    "    one_hot_col_name = ['building_class']\n",
    "    cont_col_name = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                     'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                     'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                     'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                     'pct_masters_degree','walk_score','bike_score']\n",
    "\n",
    "    print('doing one-hot...')\n",
    "    dum_dat = onehotdat(dat,one_hot_col_name,False)\n",
    "    print(len(dum_dat))\n",
    "    \n",
    "    print('extract continuous...')\n",
    "    cont_dat = dat[cont_col_name].fillna(value=0).astype(float)\n",
    "    print(len(cont_dat))\n",
    "    \n",
    "    res_dat = dat[['atlas_location_uuid']].join([cont_dat,dum_dat],how='left')\n",
    "    print(len(res_dat))\n",
    "    assert(len(res_dat)==len(dum_dat))\n",
    "    assert(len(res_dat)==len(cont_dat))\n",
    "    return {'data':res_dat,\n",
    "            'cont_feat_num':len(list(cont_dat.columns)),\n",
    "            'dum_feat_num':len(list(dum_dat.columns))}\n",
    "\n",
    "def comp_transpd2np(featdat,trdat,ttdat,not_col_name):\n",
    "    tr_feat = pd.merge(trdat,featdat,on='duns_number',how='inner')\n",
    "#     print(col_list)\n",
    "    col_list = [ n for n in list(tr_feat.columns) if n not in not_col_name ] \n",
    "    trainX = tr_feat.loc[:,col_list].to_numpy()\n",
    "    trainY = tr_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    \n",
    "    tt_feat = pd.merge(ttdat,featdat,on='duns_number',how='inner')\n",
    "    col_list = [ n for n in list(tt_feat.columns) if n not in not_col_name ] \n",
    "#     print(col_list)\n",
    "    testX = tt_feat.loc[:,col_list].to_numpy()\n",
    "    testY = tt_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    return trainX,trainY,testX,testY\n",
    "\n",
    "def transpd2np(featdatC,featdatL,pairdat,cont_col_nameC,cont_col_nameL,not_feat_col):\n",
    "    tr_feat = pd.merge(pairdat,featdatC,on='duns_number',how='inner')\n",
    "    XCC = tr_feat.loc[:,cont_col_nameC].to_numpy()\n",
    "    out_col = []\n",
    "    out_col.extend(not_feat_col)\n",
    "    out_col.extend(cont_col_nameC)\n",
    "    dum_col_nameC = [col for col in list(tr_feat.columns) if col not in out_col]\n",
    "    XDC = tr_feat.loc[:,dum_col_nameC].to_numpy()\n",
    "\n",
    "    tr_feat = pd.merge(pairdat,featdatL,on='atlas_location_uuid',how='inner')\n",
    "    XCL = tr_feat.loc[:,cont_col_nameL].to_numpy()\n",
    "    out_col = []\n",
    "    out_col.extend(not_feat_col)\n",
    "    out_col.extend(cont_col_nameL)\n",
    "    dum_col_nameL = [col for col in list(tr_feat.columns) if col not in out_col]\n",
    "    XDL = tr_feat.loc[:,dum_col_nameL].to_numpy()\n",
    "\n",
    "    Y = pairdat[['label']].to_numpy()\n",
    "    return XCC,XDC,XCL,XDL,Y\n",
    "\n",
    "def transpd2np_train_test(featdatC,featdatL,trdat,ttdat):\n",
    "    not_feat_col = ['duns_number',\n",
    "                     'atlas_location_uuid',\n",
    "                     'longitude_loc',\n",
    "                     'latitude_loc',\n",
    "                     'label']\n",
    "    cont_col_nameC = ['emp_here','emp_total','sales_volume_us','square_footage','emp_here_range']\n",
    "    cont_col_nameL = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                     'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                     'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                     'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                     'pct_masters_degree','walk_score','bike_score']\n",
    "    trXCC,trXDC,trXCL,trXDL,trY = transpd2np(featdatC,featdatL,trdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    ttXCC,ttXDC,ttXCL,ttXDL,ttY = transpd2np(featdatC,featdatL,ttdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    \n",
    "    trXC = np.concatenate([trXCC,trXCL],axis=1)\n",
    "    trXD = np.concatenate([trXDC,trXDL],axis=1)\n",
    "    ttXC = np.concatenate([ttXCC,ttXCL],axis=1)\n",
    "    ttXD = np.concatenate([ttXDC,ttXDL],axis=1)\n",
    "#     trXC = 1.0*trXCC\n",
    "#     trXD = 1.0*trXDC\n",
    "#     ttXC = 1.0*ttXCC\n",
    "#     ttXD = 1.0*ttXDC\n",
    "    del trXCC,trXDC,trXCL,trXDL,ttXCC,ttXDC,ttXCL,ttXDL\n",
    "    return trXC,trXD,ttXC,ttXD,trY,ttY\n",
    "\n",
    "def transpdfeat_w_pair(featdat,pairdat,key_col,not_col_name):\n",
    "    tr_feat = pd.merge(pairdat,featdat,on=key_col,how='inner').fillna(0)\n",
    "    feat_col_name = [col for col in list(tr_feat.columns) if col not in not_col_name]\n",
    "    X = tr_feat.loc[:,feat_col_name].to_numpy()\n",
    "    return X\n",
    "\n",
    "def normalize_dat_v2(trX,ttX,axis=0):\n",
    "    center = trX.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = trX.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    trX = (trX-center)/scale\n",
    "    ttX = (ttX-center)/scale\n",
    "    return trX,ttX\n",
    "\n",
    "def get_para_normalize_dat(trX,axis=0):\n",
    "    center = trX.mean(axis=axis)\n",
    "    scale = trX.std(axis=axis)\n",
    "    scale += 1e-4\n",
    "    return center,scale\n",
    "\n",
    "def apply_para_normalize_dat(X,center,scale,axis=0):\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    X = (X-center)/scale\n",
    "    return X\n",
    "\n",
    "def normalize_dat(trX,ttX,cols=5,axis=0):\n",
    "    D = trX[:,:cols]\n",
    "    center = D.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = D.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    trX[:,:cols] = (D-center)/scale\n",
    "    ttX[:,:cols] = (ttX[:,:cols]-center)/scale\n",
    "    \n",
    "def calc_topk_acc_v2(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    \"\"\"\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    match_array = np.logical_or.reduce(max_k_cat==y_truth_cat, axis=1) #得到匹配结果\n",
    "    topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "    return topk_acc_score\n",
    "\n",
    "def calc_topk_acc_cat_all(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    return top1-topk acc\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    M = max_k_cat==y_truth_cat\n",
    "    for k in range(M.shape[1]):\n",
    "        match_array = np.logical_or.reduce(M[:,:k+1], axis=1) #得到匹配结果\n",
    "        topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "        res.append(topk_acc_score)\n",
    "    return res\n",
    "\n",
    "def transpd2np_single(featdatC,featdatL,trdat):\n",
    "    not_feat_col = ['duns_number',\n",
    "                     'atlas_location_uuid',\n",
    "                     'longitude_loc',\n",
    "                     'latitude_loc',\n",
    "                     'label']\n",
    "    cont_col_nameC = ['emp_here','emp_total','sales_volume_us','square_footage','emp_here_range']\n",
    "    cont_col_nameL = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                     'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                     'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                     'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                     'pct_masters_degree','walk_score','bike_score']\n",
    "    trXCC,trXDC,trXCL,trXDL,trY = transpd2np(featdatC,featdatL,trdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    \n",
    "    trXC = np.concatenate([trXCC,trXCL],axis=1)\n",
    "    trXD = np.concatenate([trXDC,trXDL],axis=1)\n",
    "    del trXCC,trXDC,trXCL,trXDL\n",
    "    return trXC,trXD,trY\n",
    "\n",
    "def onehot2cat(x):\n",
    "    \"\"\"\n",
    "    x: each row is a sample\n",
    "    \"\"\"\n",
    "    return [np.where(r==1)[0][0] for r in x]\n",
    "\n",
    "def get_loc_feat_by_comp(proc_comp_dat,pair_dat):\n",
    "    tr_feat = pd.merge(pair_dat[['atlas_location_uuid','duns_number']],proc_comp_dat,on='duns_number',how='inner')\n",
    "#     tr_feat = tr_feat.fillna(0)\n",
    "    tr_feat = tr_feat.groupby(['atlas_location_uuid']).mean().drop(columns=['duns_number'])\n",
    "    return tr_feat\n",
    "\n",
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (11,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating train_val_test csv\n",
      "6181\n",
      "Neg dat num: 12362 ;Pos dat num: 6219\n",
      "Train dat: 14865 Test dat: 3702\n",
      "8704\n",
      "train_val_test_location_company Done\n",
      "generating train_val_test csv\n",
      "56367\n",
      "Neg dat num: 112734 ;Pos dat num: 56490\n",
      "Train dat: 135379 Test dat: 33797\n",
      "79030\n",
      "train_val_test_location_company Done\n",
      "generating train_val_test csv\n",
      "24947\n",
      "Neg dat num: 49894 ;Pos dat num: 25040\n",
      "Train dat: 59947 Test dat: 14960\n",
      "35016\n",
      "train_val_test_location_company Done\n",
      "generating train_val_test csv\n",
      "90584\n",
      "Neg dat num: 181168 ;Pos dat num: 90786\n",
      "Train dat: 217563 Test dat: 54308\n",
      "127091\n",
      "train_val_test_location_company Done\n",
      "generating train_val_test csv\n",
      "106667\n",
      "Neg dat num: 213334 ;Pos dat num: 106780\n",
      "Train dat: 256091 Test dat: 63970\n",
      "149228\n",
      "train_val_test_location_company Done\n",
      "doing one-hot...\n",
      "5863\n",
      "extract continuous...\n",
      "5863\n",
      "5863\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "418148\n",
      "start saving company and location feature...\n",
      "2229211.9267450343\n",
      "0.04347826086956536\n",
      "(418148, 103)\n",
      "Done\n",
      "All Done\n"
     ]
    }
   ],
   "source": [
    "##Multi training data generator(multi city)\n",
    "#如果不合并所有数据在进行dummy 会出现一些category在某些城市不出现的情况，从而导致问题\n",
    "#8-2分训练测试集\n",
    "\n",
    "def transpd2np_single(featdat,cont_col_name:list,not_feat_col:list,id_col_name:list):\n",
    "    XC = featdat.loc[:,cont_col_name].to_numpy()\n",
    "    out_col = not_feat_col+cont_col_name\n",
    "    dum_col_name = [col for col in list(featdat.columns) if col not in out_col]\n",
    "    XD = featdat.loc[:,dum_col_name].to_numpy()\n",
    "    Y = featdat[id_col_name].to_numpy()\n",
    "    return XC,XD,Y,cont_col_name,dum_col_name,id_col_name\n",
    "\n",
    "datapath = '/Users/yefeichen/Database/location_recommender_system/'\n",
    "cfile = ['dnb_pa.csv','dnb_sf.csv','dnb_sj.csv','dnb_Los_Angeles.csv','dnb_New_York.csv']\n",
    "lfile = 'location_scorecard_191113.csv'\n",
    "clfile = ['PA_191113.csv','SF_191113.csv','SJ_191113.csv','LA_191113.csv','NY_191113.csv']\n",
    "\n",
    "not_feat_col = ['duns_number',\n",
    "                 'atlas_location_uuid',\n",
    "                 'longitude_loc',\n",
    "                 'latitude_loc',\n",
    "                 'label']\n",
    "cont_col_nameC = ['emp_here','emp_total','sales_volume_us','square_footage','emp_here_range']\n",
    "cont_col_nameL = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                 'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                 'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                 'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                 'pct_masters_degree','walk_score','bike_score']\n",
    "key_col_comp = ['duns_number']\n",
    "key_col_loc = ['atlas_location_uuid']\n",
    "\n",
    "ind_city = 0\n",
    "\n",
    "train_test_val_pairs = []\n",
    "dat_comp_pds = []\n",
    "dat_loc_pds = []\n",
    "\n",
    "pdlls = [] #all location feat pd list\n",
    "pdccs = []\n",
    "for ind_city in range(5):\n",
    "    pdc = pd.read_csv(pjoin(datapath,cfile[ind_city]))\n",
    "    pdl = pd.read_csv(pjoin(datapath,lfile))\n",
    "    pdcl = pd.read_csv(pjoin(datapath,clfile[ind_city]))\n",
    "    \n",
    "    print('generating train_val_test csv')\n",
    "    #train_test_val_pairs :[ duns_number, atlas_location_uuid, label, city, fold ]\n",
    "    pair_dat = getPosNegdat(pdcl)\n",
    "    tr,tt = splitdat(pair_dat,key_column=['duns_number','atlas_location_uuid'],right_colunm='label_tr',rate_tr=0.8)\n",
    "    #training pair ==> pair format with positive only\n",
    "    train_pos_pair = tr[tr['label']==1].groupby(['duns_number','atlas_location_uuid','label']).first().reset_index()[['duns_number','atlas_location_uuid','label']]\n",
    "    #testing pair ==> pair format with positive and negative both\n",
    "    testing_pair = tt.reset_index()[['duns_number','atlas_location_uuid','label']]\n",
    "    \n",
    "    train_pos_pair['fold'] = 0\n",
    "    testing_pair['fold'] = 2\n",
    "\n",
    "    train_test_val_pair = pd.concat([train_pos_pair,testing_pair])\n",
    "    train_test_val_pair['city'] = ind_city\n",
    "    train_test_val_pairs.append(train_test_val_pair)\n",
    "    print(len(train_test_val_pair))\n",
    "    print('train_val_test_location_company Done')\n",
    "    \n",
    "    #building features\n",
    "    col_list = list(pdl.columns)\n",
    "    pdll = pdl.merge(pdcl,how='inner',on=['atlas_location_uuid'],suffixes=['','_right'])\n",
    "    pdll = pdll[pdll['duns_number'].isnull()==False]\n",
    "    pdll = pdll.groupby(['atlas_location_uuid']).first().reset_index()\n",
    "    pdll = pdll[col_list]\n",
    "    pdlls.append(pdll)\n",
    "    \n",
    "    #company feature\n",
    "    pdccs.append(pdc)\n",
    "    \n",
    "#for loop end\n",
    "pdlls = pd.concat(pdlls,axis=0)\n",
    "pdccs = pd.concat(pdccs,axis=0)\n",
    "\n",
    "    \n",
    "#building feature\n",
    "pdlls = pdlls.reset_index()\n",
    "proc_pdl = location_dat_process(pdlls)\n",
    "\n",
    "#company feature\n",
    "pdccs = pdccs.reset_index()\n",
    "proc_pdc = comp_dat_process(pdccs)\n",
    "print(len(proc_pdc))\n",
    "    \n",
    "\n",
    "print('start saving company and location feature...')\n",
    "\n",
    "XC_comp,XD_comp,Y_comp,c_comp_name,d_comp_name,y_comp_name = transpd2np_single(proc_pdc,cont_col_nameC,not_feat_col,id_col_name=key_col_comp)\n",
    "XC_loc,XD_loc,Y_loc,c_loc_name,d_loc_name,y_loc_name = transpd2np_single(proc_pdl['data'],cont_col_nameL,not_feat_col,id_col_name=key_col_loc)\n",
    "\n",
    "C_comp,S_comp = get_para_normalize_dat(XC_comp)\n",
    "C_loc,S_loc = get_para_normalize_dat(XC_loc)\n",
    "XC_comp = apply_para_normalize_dat(XC_comp,C_comp,S_comp)\n",
    "XC_loc = apply_para_normalize_dat(XC_loc,C_loc,S_loc)\n",
    "    \n",
    "X_comp = np.concatenate([Y_comp,XC_comp,XD_comp],axis=1)\n",
    "X_loc = np.concatenate([Y_loc,XC_loc,XD_loc],axis=1)\n",
    "\n",
    "comp_norm_param = {\n",
    "    'C_comp':C_comp,\n",
    "    'S_comp':S_comp,\n",
    "    'columns':c_comp_name\n",
    "}\n",
    "\n",
    "loc_norm_param = {\n",
    "    'C_loc':C_loc,\n",
    "    'S_loc':S_loc,\n",
    "    'columns':c_loc_name\n",
    "}\n",
    "\n",
    "save_obj(comp_norm_param,'comp_feat_norm_param3')\n",
    "save_obj(loc_norm_param,'loc_feat_norm_param3')\n",
    "\n",
    "dat_comp_pd = pd.DataFrame(data=X_comp,columns=y_comp_name+c_comp_name+d_comp_name)\n",
    "dat_loc_pd = pd.DataFrame(data=X_loc,columns=y_loc_name+c_loc_name+d_loc_name)\n",
    "\n",
    "    \n",
    "print(dat_comp_pd.to_numpy().mean())\n",
    "print(dat_loc_pd.to_numpy()[:,1:].mean())\n",
    "print(dat_comp_pd.shape)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "# print('Final merge...')\n",
    "train_test_val_pair = pd.concat(train_test_val_pairs)\n",
    "\n",
    "train_test_val_pair.to_csv('train_val_test_location_company_82split_191113.csv')\n",
    "dat_comp_pd.to_csv('company_feat3.csv')\n",
    "dat_loc_pd.to_csv('location_feat3.csv')\n",
    "print('All Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((418148, 103), (5863, 24))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_comp_pd.shape,dat_loc_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
