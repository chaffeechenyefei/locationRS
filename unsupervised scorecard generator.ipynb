{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate affinity score between a company and location by unsupervised learning.\n",
    "# Each building is represented by the features of companies inside it.\n",
    "# It just like the task of face recognition, we take several photoes for each person and use these photoes to represent that person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe each location with companies in side\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pygeohash as pgh\n",
    "from math import *\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "pjoin = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function_base\n",
    "def splitdat(dat,key_column=['duns_number'],right_colunm='atlas_location_uuid_tr',rate_tr=0.8):\n",
    "    \"\"\"\n",
    "    split the <company,location> pair into training/testing dat\n",
    "    \"\"\"\n",
    "    tr = dat.sample(frac=rate_tr)\n",
    "    tt = pd.merge(dat,tr,on=key_column,how='left',suffixes=['','_tr'])\n",
    "    tt = tt[tt[right_colunm].isnull()]\n",
    "    tt = tt[list(tr.columns)]\n",
    "    print('Train dat:', len(tr), 'Test dat:', len(tt))\n",
    "    return tr,tt\n",
    "\n",
    "#data process\n",
    "def onehotdat(dat,key_column:list):\n",
    "    dat[key_column] = dat[key_column].astype(str)\n",
    "    dum_dat = pd.get_dummies(dat[key_column],dummy_na=True)\n",
    "    return dum_dat\n",
    "\n",
    "def split2num(emp_range:str):\n",
    "    max_emp_val = emp_range.replace(' ','').split('-')\n",
    "    if len(max_emp_val)<2:\n",
    "        return 10\n",
    "    else:\n",
    "        return float(max_emp_val[1])\n",
    "    \n",
    "def max_col(dat,col,minval=1):\n",
    "    dat[col] = dat[col].apply(lambda r:max(r,minval))\n",
    "\n",
    "def comp_dat_process(dat):\n",
    "    \"\"\"\n",
    "    pd -> company key,cont_feature,spec_feature,dum_feature\n",
    "    \"\"\"\n",
    "    one_hot_col_name = ['major_industry_category','location_type','primary_sic_2_digit']\n",
    "    spec_col_name = 'emp_here_range'\n",
    "    cont_col_name = ['emp_here','emp_total','sales_volume_us','square_footage']\n",
    "\n",
    "    print('doing one-hot...')\n",
    "    dum_dat = onehotdat(dat,one_hot_col_name)\n",
    "    \n",
    "    print('extract continuous...')\n",
    "    cont_dat = dat[cont_col_name].fillna(value=0).astype(float)\n",
    "    \n",
    "    print('specific feature')\n",
    "    spec_dat = dat[spec_col_name].fillna(value='1-10').astype(str)\n",
    "    spec_dat = spec_dat.apply(lambda row: split2num(row))\n",
    "    \n",
    "    max_col(cont_dat,'emp_here',1)\n",
    "    \n",
    "    res_dat = dat[['duns_number']].join([cont_dat,spec_dat,dum_dat],how='left')\n",
    "    assert(len(res_dat)==len(dum_dat))\n",
    "    assert(len(res_dat)==len(cont_dat))\n",
    "    assert(len(res_dat)==len(spec_dat))\n",
    "    return res_dat\n",
    "\n",
    "def comp_transpd2np(featdat,trdat,ttdat,not_col_name):\n",
    "    tr_feat = pd.merge(trdat,featdat,on='duns_number',how='inner')\n",
    "#     print(col_list)\n",
    "    col_list = [ n for n in list(tr_feat.columns) if n not in not_col_name ] \n",
    "    trainX = tr_feat.loc[:,col_list].to_numpy()\n",
    "    trainY = tr_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    \n",
    "    tt_feat = pd.merge(ttdat,featdat,on='duns_number',how='inner')\n",
    "    col_list = [ n for n in list(tt_feat.columns) if n not in not_col_name ] \n",
    "#     print(col_list)\n",
    "    testX = tt_feat.loc[:,col_list].to_numpy()\n",
    "    testY = tt_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    return trainX,trainY,testX,testY\n",
    "\n",
    "def normalize_dat(trX,ttX,cols=5,axis=0):\n",
    "    D = trX[:,:cols]\n",
    "    center = D.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = D.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    trX[:,:cols] = (D-center)/scale\n",
    "    ttX[:,:cols] = (ttX[:,:cols]-center)/scale\n",
    "    \n",
    "def normalize_dat_single(X,cols=5,axis=0):\n",
    "    D = X[:,:cols]\n",
    "    center = D.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = D.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    X[:,:cols] = (D-center)/scale\n",
    "    return X,center,scale\n",
    "    \n",
    "def calc_topk_acc_v2(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    \"\"\"\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    match_array = np.logical_or.reduce(max_k_cat==y_truth_cat, axis=1) #得到匹配结果\n",
    "    topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "    return topk_acc_score\n",
    "\n",
    "def calc_topk_acc_cat_all(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    return top1-topk acc\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    M = max_k_cat==y_truth_cat\n",
    "    for k in range(M.shape[1]):\n",
    "        match_array = np.logical_or.reduce(M[:,:k+1], axis=1) #得到匹配结果\n",
    "        topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "        res.append(topk_acc_score)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load\n",
    "datapath = '/Users/yefeichen/Database/location_recommender_system/'\n",
    "cfile = ['dnb_pa.csv','dnb_sf.csv','dnb_sj.csv']\n",
    "lfile = 'location_scorecard_190912.csv'\n",
    "clfile = ['PA.csv','SF.csv','SJ.csv']\n",
    "savefile = ['PA_comp_loc_score.csv','SF_comp_loc_score.csv','SJ_comp_loc_score.csv']\n",
    "\n",
    "ind_city = 0\n",
    "\n",
    "pdc = pd.read_csv(pjoin(datapath,cfile[ind_city]))\n",
    "pdl = pd.read_csv(pjoin(datapath,lfile))\n",
    "pdcl = pd.read_csv(pjoin(datapath,clfile[ind_city]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company number=6219, location number=352\n",
      "dummy and get data feat\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "transfer 2 numpy\n",
      "Y_comp_number = (6219, 1)\n",
      "(6219, 92)\n",
      "data reformat\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e51a3efe64e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mresM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrossFeatM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp_num\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloc_num\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mresM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# resM.to_csv(savefile[ind_city])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#generate cross feature of company and location for building\n",
    "def comp_transpd2np_single(featdat,trdat,not_col_name):\n",
    "    tr_feat = pd.merge(trdat,featdat,on='duns_number',how='inner')\n",
    "    col_list = [ n for n in list(tr_feat.columns) if n not in not_col_name ] \n",
    "    X = tr_feat.loc[:,col_list].to_numpy()\n",
    "    Y_comp = tr_feat[['duns_number']].to_numpy()\n",
    "    Y_loc = tr_feat[['atlas_location_uuid']].to_numpy()\n",
    "    \n",
    "    return X,Y_comp,Y_loc\n",
    "\n",
    "not_col_name = ['duns_number','atlas_location_uuid','geo_distance','longitude_loc','latitude_loc']\n",
    "\n",
    "location_cnt = pdcl.groupby('atlas_location_uuid').first()\n",
    "comp_num = pdcl.shape[0]\n",
    "loc_num = len(location_cnt)\n",
    "print('company number=%d, location number=%d'%(comp_num,loc_num))\n",
    "\n",
    "print('dummy and get data feat')\n",
    "proc_pdc = comp_dat_process(pdc)\n",
    "print('transfer 2 numpy')\n",
    "X,Y_comp,Y_loc = comp_transpd2np_single(proc_pdc,pdcl,not_col_name=not_col_name)\n",
    "X,_,_ = normalize_dat_single(X,cols=5)\n",
    "print('Y_comp_number =', Y_comp.shape)\n",
    "print(X.shape)\n",
    "\n",
    "print('data reformat')\n",
    "# Y_comp_Y = np.tile(Y_comp,(1,Y_comp.shape[0])).reshape(-1,1)\n",
    "# Y_loc_Y = np.tile(Y_loc,(1,Y_loc.shape[0])).transpose().reshape(-1,1)\n",
    "# Y_comp = Y_comp.reshape(-1)\n",
    "\n",
    "\n",
    "# assert(comp_num*comp_num==len(M))\n",
    "resM = None\n",
    "step = comp_num\n",
    "n = 100\n",
    "N = int(comp_num/n) + 1\n",
    "\n",
    "for k in range(N):\n",
    "    break\n",
    "    if k%10==1:\n",
    "        print('percentage:%0.2f'%(k/N))\n",
    "    inds = k*n\n",
    "    inde = min((k+1)*n,comp_num)\n",
    "    \n",
    "    if inds >= comp_num:\n",
    "        break\n",
    "        \n",
    "#     print('cal cross distance')\n",
    "    distQR = euclidean_distances(X[inds:inde,:],X)\n",
    "    distQR = distQR.reshape(-1,1)\n",
    "#     print('data merge')\n",
    "    Y_comp_tmp = Y_comp[inds:inde,:]\n",
    "    #those things are very confusing. after all we struggled out.\n",
    "    Y_comp_Y = np.tile(Y_comp_tmp,(1,Y_comp.shape[0])).reshape(-1,1)\n",
    "    Y_loc_Y = np.tile(Y_loc,(1,Y_comp_tmp.shape[0])).transpose().reshape(-1,1)\n",
    "    assert(Y_loc_Y.shape[0]==distQR.shape[0])\n",
    "    assert(Y_comp_Y.shape[0]==distQR.shape[0])\n",
    "    M = np.concatenate([Y_comp_Y,Y_loc_Y,distQR],axis=1)\n",
    "    \n",
    "#     print('create data')\n",
    "    pdM = pd.DataFrame(data=M,columns=['duns_number','atlas_location_uuid','sim_score'])\n",
    "#     pdM['sim_score_new'] = pdM.sim_score.apply(lambda r: 1e6 if r<0.05 else r)\n",
    "\n",
    "    \n",
    "    if resM is None:\n",
    "        resM = pdM.groupby(['duns_number','atlas_location_uuid'])[['sim_score']].min()\n",
    "    else:\n",
    "        crossFeatM = pdM.groupby(['duns_number','atlas_location_uuid'])[['sim_score']].min()\n",
    "        resM = pd.concat([resM,crossFeatM],axis=0)\n",
    "        \n",
    "assert(comp_num*loc_num==resM.shape[0])\n",
    "\n",
    "# resM.to_csv(savefile[ind_city])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company number=25040, location number=614\n",
      "dummy and get data feat\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "transfer 2 numpy\n",
      "Y_comp_number = (25040, 1)\n",
      "data reformat\n",
      "percentage:0.00\n",
      "percentage:0.02\n",
      "percentage:0.04\n",
      "percentage:0.06\n",
      "percentage:0.08\n",
      "percentage:0.10\n",
      "percentage:0.12\n",
      "percentage:0.14\n",
      "percentage:0.16\n",
      "percentage:0.18\n",
      "percentage:0.20\n",
      "percentage:0.22\n",
      "percentage:0.24\n",
      "percentage:0.25\n",
      "percentage:0.27\n",
      "percentage:0.29\n",
      "percentage:0.31\n",
      "percentage:0.33\n",
      "percentage:0.35\n",
      "percentage:0.37\n",
      "percentage:0.39\n",
      "percentage:0.41\n",
      "percentage:0.43\n",
      "percentage:0.45\n",
      "percentage:0.47\n",
      "percentage:0.49\n",
      "percentage:0.51\n",
      "percentage:0.53\n",
      "percentage:0.55\n",
      "percentage:0.57\n",
      "percentage:0.59\n",
      "percentage:0.61\n",
      "percentage:0.63\n",
      "percentage:0.65\n",
      "percentage:0.67\n",
      "percentage:0.69\n",
      "percentage:0.71\n",
      "percentage:0.73\n",
      "percentage:0.75\n",
      "percentage:0.76\n",
      "percentage:0.78\n",
      "percentage:0.80\n",
      "percentage:0.82\n",
      "percentage:0.84\n",
      "percentage:0.86\n",
      "percentage:0.88\n",
      "percentage:0.90\n",
      "percentage:0.92\n",
      "percentage:0.94\n",
      "percentage:0.96\n",
      "percentage:0.98\n"
     ]
    }
   ],
   "source": [
    "# Generate affinity score between a company and location by unsupervised learning.\n",
    "# Each building is represented by the features of companies inside it.\n",
    "# It just like the task of face recognition, we take several photoes for each person and use these photoes to represent that person.\n",
    "#describe each location with companies in side\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "#import pygeohash as pgh\n",
    "from math import *\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "pjoin = os.path.join\n",
    "\n",
    "#data load\n",
    "datapath = '/Users/yefeichen/Database/location_recommender_system/'\n",
    "cfile = ['dnb_pa.csv','dnb_sf.csv','dnb_sj.csv']\n",
    "lfile = 'location_scorecard_190912.csv'\n",
    "clfile = ['PA.csv','SF.csv','SJ.csv']\n",
    "savefile = ['PA_comp_loc_score.csv','SF_comp_loc_score.csv','SJ_comp_loc_score.csv']\n",
    "\n",
    "ind_city = 2\n",
    "\n",
    "pdc = pd.read_csv(pjoin(datapath,cfile[ind_city]))\n",
    "pdl = pd.read_csv(pjoin(datapath,lfile))\n",
    "pdcl = pd.read_csv(pjoin(datapath,clfile[ind_city]))\n",
    "\n",
    "#function_base\n",
    "def splitdat(dat,key_column=['duns_number'],right_colunm='atlas_location_uuid_tr',rate_tr=0.8):\n",
    "    \"\"\"\n",
    "    split the <company,location> pair into training/testing dat\n",
    "    \"\"\"\n",
    "    tr = dat.sample(frac=rate_tr)\n",
    "    tt = pd.merge(dat,tr,on=key_column,how='left',suffixes=['','_tr'])\n",
    "    tt = tt[tt[right_colunm].isnull()]\n",
    "    tt = tt[list(tr.columns)]\n",
    "    print('Train dat:', len(tr), 'Test dat:', len(tt))\n",
    "    return tr,tt\n",
    "\n",
    "#data process\n",
    "def onehotdat(dat,key_column:list):\n",
    "    dat[key_column] = dat[key_column].astype(str)\n",
    "    dum_dat = pd.get_dummies(dat[key_column],dummy_na=True)\n",
    "    return dum_dat\n",
    "\n",
    "def split2num(emp_range:str):\n",
    "    max_emp_val = emp_range.replace(' ','').split('-')\n",
    "    if len(max_emp_val)<2:\n",
    "        return 10\n",
    "    else:\n",
    "        return float(max_emp_val[1])\n",
    "    \n",
    "def max_col(dat,col,minval=1):\n",
    "    dat[col] = dat[col].apply(lambda r:max(r,minval))\n",
    "\n",
    "def comp_dat_process(dat):\n",
    "    \"\"\"\n",
    "    pd -> company key,cont_feature,spec_feature,dum_feature\n",
    "    \"\"\"\n",
    "    one_hot_col_name = ['major_industry_category','location_type','primary_sic_2_digit']\n",
    "    spec_col_name = 'emp_here_range'\n",
    "    cont_col_name = ['emp_here','emp_total','sales_volume_us','square_footage']\n",
    "\n",
    "    print('doing one-hot...')\n",
    "    dum_dat = onehotdat(dat,one_hot_col_name)\n",
    "    \n",
    "    print('extract continuous...')\n",
    "    cont_dat = dat[cont_col_name].fillna(value=0).astype(float)\n",
    "    \n",
    "    print('specific feature')\n",
    "    spec_dat = dat[spec_col_name].fillna(value='1-10').astype(str)\n",
    "    spec_dat = spec_dat.apply(lambda row: split2num(row))\n",
    "    \n",
    "    max_col(cont_dat,'emp_here',1)\n",
    "    \n",
    "    res_dat = dat[['duns_number']].join([cont_dat,spec_dat,dum_dat],how='left')\n",
    "    assert(len(res_dat)==len(dum_dat))\n",
    "    assert(len(res_dat)==len(cont_dat))\n",
    "    assert(len(res_dat)==len(spec_dat))\n",
    "    return res_dat\n",
    "\n",
    "def comp_transpd2np(featdat,trdat,ttdat,not_col_name):\n",
    "    tr_feat = pd.merge(trdat,featdat,on='duns_number',how='inner')\n",
    "#     print(col_list)\n",
    "    col_list = [ n for n in list(tr_feat.columns) if n not in not_col_name ] \n",
    "    trainX = tr_feat.loc[:,col_list].to_numpy()\n",
    "    trainY = tr_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    \n",
    "    tt_feat = pd.merge(ttdat,featdat,on='duns_number',how='inner')\n",
    "    col_list = [ n for n in list(tt_feat.columns) if n not in not_col_name ] \n",
    "#     print(col_list)\n",
    "    testX = tt_feat.loc[:,col_list].to_numpy()\n",
    "    testY = tt_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    return trainX,trainY,testX,testY\n",
    "\n",
    "def normalize_dat(trX,ttX,cols=5,axis=0):\n",
    "    D = trX[:,:cols]\n",
    "    center = D.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = D.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    trX[:,:cols] = (D-center)/scale\n",
    "    ttX[:,:cols] = (ttX[:,:cols]-center)/scale\n",
    "    \n",
    "def normalize_dat_single(X,cols=5,axis=0):\n",
    "    D = X[:,:cols]\n",
    "    center = D.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = D.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    X[:,:cols] = (D-center)/scale\n",
    "    return X,center,scale\n",
    "    \n",
    "def calc_topk_acc_v2(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    \"\"\"\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    match_array = np.logical_or.reduce(max_k_cat==y_truth_cat, axis=1) #得到匹配结果\n",
    "    topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "    return topk_acc_score\n",
    "\n",
    "def calc_topk_acc_cat_all(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    return top1-topk acc\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    M = max_k_cat==y_truth_cat\n",
    "    for k in range(M.shape[1]):\n",
    "        match_array = np.logical_or.reduce(M[:,:k+1], axis=1) #得到匹配结果\n",
    "        topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "        res.append(topk_acc_score)\n",
    "    return res\n",
    "\n",
    "#generate cross feature of company and location for building\n",
    "def comp_transpd2np_single(featdat,trdat,not_col_name):\n",
    "    tr_feat = pd.merge(trdat,featdat,on='duns_number',how='inner')\n",
    "    col_list = [ n for n in list(tr_feat.columns) if n not in not_col_name ] \n",
    "    X = tr_feat.loc[:,col_list].to_numpy()\n",
    "    Y_comp = tr_feat[['duns_number']].to_numpy()\n",
    "    Y_loc = tr_feat[['atlas_location_uuid']].to_numpy()\n",
    "    \n",
    "    return X,Y_comp,Y_loc\n",
    "\n",
    "not_col_name = ['duns_number','atlas_location_uuid','geo_distance','longitude_loc','latitude_loc']\n",
    "\n",
    "location_cnt = pdcl.groupby('atlas_location_uuid').first()\n",
    "comp_num = pdcl.shape[0]\n",
    "loc_num = len(location_cnt)\n",
    "print('company number=%d, location number=%d'%(comp_num,loc_num))\n",
    "\n",
    "print('dummy and get data feat')\n",
    "proc_pdc = comp_dat_process(pdc)\n",
    "print('transfer 2 numpy')\n",
    "X,Y_comp,Y_loc = comp_transpd2np_single(proc_pdc,pdcl,not_col_name=not_col_name)\n",
    "X,_,_ = normalize_dat_single(X,cols=5)\n",
    "print('Y_comp_number =', Y_comp.shape)\n",
    "\n",
    "\n",
    "print('data reformat')\n",
    "#Y_comp_Y = np.tile(Y_comp,(1,Y_comp.shape[0])).reshape(-1,1)\n",
    "#Y_loc_Y = np.tile(Y_loc,(1,Y_loc.shape[0])).transpose().reshape(-1,1)\n",
    "# Y_comp = Y_comp.reshape(-1)\n",
    "\n",
    "# assert(comp_num*comp_num==len(M))\n",
    "resM = None\n",
    "step = comp_num\n",
    "n = 500\n",
    "N = ceil(comp_num/n)\n",
    "\n",
    "for k in range(N):\n",
    "    print('percentage:%0.2f'%(k/N))\n",
    "    inds = k*n\n",
    "    inde = min((k+1)*n,comp_num)\n",
    "    \n",
    "    if inds >= comp_num:\n",
    "        break\n",
    "        \n",
    "#     print('cal cross distance')\n",
    "    distQR = euclidean_distances(X[inds:inde,:],X)\n",
    "    distQR = distQR.reshape(-1,1)\n",
    "    distQR[distQR<0.001]=1e3\n",
    "#     print('data merge')\n",
    "    #M = np.concatenate([Y_comp_Y[inds*step:inde*step],Y_loc_Y[inds*step:inde*step],distQR],axis=1)\n",
    "    Y_comp_tmp = Y_comp[inds:inde,:]\n",
    "    #Y_loc_tmp = Y_loc[inds:inde,:]\n",
    "    Y_comp_Y = np.tile(Y_comp_tmp,(1,Y_comp.shape[0])).reshape(-1,1)\n",
    "    Y_loc_Y = np.tile(Y_loc,(1,Y_comp_tmp.shape[0])).transpose().reshape(-1,1)\n",
    "    M = np.concatenate([Y_comp_Y,Y_loc_Y,distQR],axis=1)\n",
    "#     print('create data')\n",
    "    pdM = pd.DataFrame(data=M,columns=['duns_number','atlas_location_uuid','sim_score'])\n",
    "#    pdM['sim_score_new'] = pdM.sim_score.apply(lambda r: 1e6 if r<0.05 else r)\n",
    "    \n",
    "    if resM is None:\n",
    "        resM = pdM.groupby(['duns_number','atlas_location_uuid'])[['sim_score']].min()\n",
    "    else:\n",
    "        crossFeatM = pdM.groupby(['duns_number','atlas_location_uuid'])[['sim_score']].min()\n",
    "        resM = pd.concat([resM,crossFeatM],axis=0)\n",
    "        \n",
    "#assert(comp_num*loc_num==resM.shape[0])\n",
    "\n",
    "resM.to_csv(savefile[ind_city])\n",
    "assert(comp_num*loc_num==resM.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15374560, 15374560, 15374560)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resK),len(resM),comp_num*loc_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "inds = k*n\n",
    "inde = min((k+1)*n,comp_num)\n",
    "Y_comp_tmp = Y_comp[inds:inde,:]\n",
    "Y_comp_Y = np.tile(Y_comp_tmp,(1,Y_comp.shape[0])).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6219"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_comp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6219, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_loc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_comp_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sim_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duns_number</th>\n",
       "      <th>atlas_location_uuid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1077028.0</th>\n",
       "      <th>000c076c-390a-4c35-7313-fca29e390ece</th>\n",
       "      <td>0.028080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>012d9fe1-5b24-93f2-1902-cd55603382ec</th>\n",
       "      <td>0.028080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01cc35f5-215d-7610-8f97-89902a8881dc</th>\n",
       "      <td>0.011799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02001210-85d1-a7d6-b2e8-0ef536bbcf7d</th>\n",
       "      <td>0.187397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0224cd59-8e48-c504-1c79-47fd5cb20d53</th>\n",
       "      <td>0.013037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sim_score\n",
       "duns_number atlas_location_uuid                            \n",
       "1077028.0   000c076c-390a-4c35-7313-fca29e390ece   0.028080\n",
       "            012d9fe1-5b24-93f2-1902-cd55603382ec   0.028080\n",
       "            01cc35f5-215d-7610-8f97-89902a8881dc   0.011799\n",
       "            02001210-85d1-a7d6-b2e8-0ef536bbcf7d   0.187397\n",
       "            0224cd59-8e48-c504-1c79-47fd5cb20d53   0.013037"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "621900/6219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pdM.groupby(['duns_number','atlas_location_uuid'])[['sim_score_new']].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35200"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_loc_Y = np.tile(Y_loc,(1,Y_comp_tmp.shape[0])).transpose().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6219, 100)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.tile(Y_loc,(1,Y_comp_tmp.shape[0]))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['77c493ec-1424-6d74-8db3-ee8fce0092db'],\n",
       "       ['77c493ec-1424-6d74-8db3-ee8fce0092db'],\n",
       "       ['7a687b49-1055-24f6-ae84-e390438fe3c4'],\n",
       "       ...,\n",
       "       ['c5b270a7-6368-6e77-6039-618cc709f3a6'],\n",
       "       ['a0e26081-e098-a151-67d3-9b7194c5e467'],\n",
       "       ['a0e26081-e098-a151-67d3-9b7194c5e467']], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_loc_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
