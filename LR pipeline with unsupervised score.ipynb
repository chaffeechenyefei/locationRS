{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe each location with companies in side\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pygeohash as pgh\n",
    "from math import *\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from matplotlib import pylab\n",
    "from sklearn.preprocessing import normalize\n",
    "pjoin = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function_base\n",
    "def getPosNegdat(dat):\n",
    "    \"\"\"\n",
    "    dat: pos pair of data (location,company,geo,distance)\n",
    "    return pos/neg pair of data, same structure of dat except one more column for label\n",
    "    \"\"\"\n",
    "    shuffle_dat = dat.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # shuffle_dat.head()\n",
    "\n",
    "    twin_dat = dat.join(shuffle_dat,how='left',lsuffix='_left',rsuffix='_right')\n",
    "    twin_dat = twin_dat[twin_dat['atlas_location_uuid_left'] != twin_dat['atlas_location_uuid_right']]\n",
    "    print(len(twin_dat))\n",
    "    twin_dat.head()\n",
    "\n",
    "    neg_datA = twin_dat[['duns_number_left','atlas_location_uuid_right','longitude_loc_right','latitude_loc_right']]\n",
    "    neg_datA = neg_datA.rename(columns={'duns_number_left':'duns_number','atlas_location_uuid_right':'atlas_location_uuid','longitude_loc_right':'longitude_loc','latitude_loc_right':'latitude_loc'})\n",
    "\n",
    "    neg_datB = twin_dat[['duns_number_right','atlas_location_uuid_left','longitude_loc_left','latitude_loc_left']]\n",
    "    neg_datB = neg_datB.rename(columns={'duns_number_right':'duns_number','atlas_location_uuid_left':'atlas_location_uuid','longitude_loc_left':'longitude_loc','latitude_loc_left':'latitude_loc'})\n",
    "\n",
    "    neg_dat = pd.concat([neg_datA,neg_datB],axis=0)\n",
    "    neg_dat['label'] = 0\n",
    "    dat['label'] = 1\n",
    "    res_dat = pd.concat([dat[['duns_number','atlas_location_uuid','longitude_loc','latitude_loc','label']],neg_dat],axis=0)\n",
    "    print('Neg dat num:',len(neg_dat),';Pos dat num:',len(dat))\n",
    "    return res_dat\n",
    "\n",
    "def splitdat(dat,key_column=['duns_number'],right_colunm='atlas_location_uuid_tr',rate_tr=0.8):\n",
    "    \"\"\"\n",
    "    split the <company,location> pair into training/testing dat\n",
    "    \"\"\"\n",
    "    tr = dat.sample(frac=rate_tr)\n",
    "    tt = pd.merge(dat,tr,on=key_column,how='left',suffixes=['','_tr'])\n",
    "    tt = tt[tt[right_colunm].isnull()]\n",
    "    tt = tt[list(tr.columns)]\n",
    "    print('Train dat:', len(tr), 'Test dat:', len(tt))\n",
    "    return tr,tt\n",
    "\n",
    "#data process\n",
    "def onehotdat(dat,key_column:list,dummy_na=True):\n",
    "    dat[key_column] = dat[key_column].astype(str)\n",
    "    dum_dat = pd.get_dummies(dat[key_column],dummy_na=dummy_na)#it has nan itself\n",
    "    return dum_dat\n",
    "\n",
    "def split2num(emp_range:str):\n",
    "    max_emp_val = emp_range.replace(' ','').split('-')\n",
    "    if len(max_emp_val)<2:\n",
    "        return 10\n",
    "    else:\n",
    "        return float(max_emp_val[1])\n",
    "    \n",
    "def max_col(dat,col,minval=1):\n",
    "    dat[col] = dat[col].apply(lambda r:max(r,minval))\n",
    "\n",
    "def comp_dat_process(dat):\n",
    "    \"\"\"\n",
    "    pd -> company key,cont_feature,spec_feature,dum_feature\n",
    "    \"\"\"\n",
    "    one_hot_col_name = ['major_industry_category','location_type','primary_sic_2_digit']\n",
    "    spec_col_name = 'emp_here_range'\n",
    "    cont_col_name = ['emp_here','emp_total','sales_volume_us','square_footage']\n",
    "\n",
    "    print('doing one-hot...')\n",
    "    dum_dat = onehotdat(dat,one_hot_col_name)\n",
    "    \n",
    "    print('extract continuous...')\n",
    "    cont_dat = dat[cont_col_name].fillna(value=0).astype(float)\n",
    "    \n",
    "    print('specific feature')\n",
    "    spec_dat = dat[spec_col_name].fillna(value='1-10').astype(str)\n",
    "    spec_dat = spec_dat.apply(lambda row: split2num(row))\n",
    "    \n",
    "    max_col(cont_dat,'emp_here',1)\n",
    "    \n",
    "    res_dat = dat[['duns_number']].join([cont_dat,spec_dat,dum_dat],how='left')\n",
    "    assert(len(res_dat)==len(dum_dat))\n",
    "    assert(len(res_dat)==len(cont_dat))\n",
    "    assert(len(res_dat)==len(spec_dat))\n",
    "    return res_dat\n",
    "\n",
    "def location_dat_process(dat):\n",
    "    \"\"\"\n",
    "    pd -> location key,cont_feature,dum_feature\n",
    "    \"\"\"\n",
    "    one_hot_col_name = ['building_class']\n",
    "    cont_col_name = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                     'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                     'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                     'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                     'pct_masters_degree','walk_score','bike_score']\n",
    "\n",
    "    print('doing one-hot...')\n",
    "    dum_dat = onehotdat(dat,one_hot_col_name,False)\n",
    "    \n",
    "    print('extract continuous...')\n",
    "    cont_dat = dat[cont_col_name].fillna(value=0).astype(float)\n",
    "    \n",
    "    res_dat = dat[['atlas_location_uuid']].join([cont_dat,dum_dat],how='left')\n",
    "    assert(len(res_dat)==len(dum_dat))\n",
    "    assert(len(res_dat)==len(cont_dat))\n",
    "    return {'data':res_dat,\n",
    "            'cont_feat_num':len(list(cont_dat.columns)),\n",
    "            'dum_feat_num':len(list(dum_dat.columns))}\n",
    "\n",
    "def comp_transpd2np(featdat,trdat,ttdat):\n",
    "    tr_feat = pd.merge(trdat,featdat,on='duns_number',how='inner')\n",
    "    col_list = list(tr_feat.columns)\n",
    "#     print(col_list)\n",
    "    trainX = tr_feat.loc[:,col_list[3:]].to_numpy()\n",
    "    trainY = tr_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    \n",
    "    tt_feat = pd.merge(ttdat,featdat,on='duns_number',how='inner')\n",
    "    col_list = list(tt_feat.columns)\n",
    "#     print(col_list)\n",
    "    testX = tt_feat.loc[:,col_list[3:]].to_numpy()\n",
    "    testY = tt_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    return trainX,trainY,testX,testY\n",
    "\n",
    "def transpd2np(featdatC,featdatL,pairdat,cont_col_nameC,cont_col_nameL,not_feat_col):\n",
    "    tr_feat = pd.merge(pairdat,featdatC,on='duns_number',how='inner')\n",
    "    XCC = tr_feat.loc[:,cont_col_nameC].to_numpy()\n",
    "    out_col = []\n",
    "    out_col.extend(not_feat_col)\n",
    "    out_col.extend(cont_col_nameC)\n",
    "    dum_col_nameC = [col for col in list(tr_feat.columns) if col not in out_col]\n",
    "    XDC = tr_feat.loc[:,dum_col_nameC]\n",
    "\n",
    "    tr_feat = pd.merge(pairdat,featdatL,on='atlas_location_uuid',how='inner')\n",
    "    XCL = tr_feat.loc[:,cont_col_nameL].to_numpy()\n",
    "    out_col = []\n",
    "    out_col.extend(not_feat_col)\n",
    "    out_col.extend(cont_col_nameL)\n",
    "    dum_col_nameL = [col for col in list(tr_feat.columns) if col not in out_col]\n",
    "    XDL = tr_feat.loc[:,dum_col_nameL]\n",
    "\n",
    "    Y = pairdat[['label']].to_numpy()\n",
    "    return XCC,XDC,XCL,XDL,Y\n",
    "\n",
    "def transpd2np_train_test(featdatC,featdatL,trdat,ttdat):\n",
    "    not_feat_col = ['duns_number',\n",
    "                     'atlas_location_uuid',\n",
    "                     'longitude_loc',\n",
    "                     'latitude_loc',\n",
    "                     'label']\n",
    "    cont_col_nameC = ['emp_here','emp_total','sales_volume_us','square_footage','emp_here_range']\n",
    "    cont_col_nameL = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                     'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                     'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                     'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                     'pct_masters_degree','walk_score','bike_score']\n",
    "    trXCC,trXDC,trXCL,trXDL,trY = transpd2np(featdatC,featdatL,trdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    ttXCC,ttXDC,ttXCL,ttXDL,ttY = transpd2np(featdatC,featdatL,ttdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    \n",
    "    trXC = np.concatenate([trXCC,trXCL],axis=1)\n",
    "    trXD = np.concatenate([trXDC,trXDL],axis=1)\n",
    "    ttXC = np.concatenate([ttXCC,ttXCL],axis=1)\n",
    "    ttXD = np.concatenate([ttXDC,ttXDL],axis=1)\n",
    "#     trXC = 1.0*trXCC\n",
    "#     trXD = 1.0*trXDC\n",
    "#     ttXC = 1.0*ttXCC\n",
    "#     ttXD = 1.0*ttXDC\n",
    "    del trXCC,trXDC,trXCL,trXDL,ttXCC,ttXDC,ttXCL,ttXDL\n",
    "    return trXC,trXD,ttXC,ttXD,trY,ttY\n",
    "\n",
    "def transpdfeat_w_pair(featdat,pairdat,key_col,not_col_name):\n",
    "    tr_feat = pd.merge(pairdat,featdat,on=key_col,how='inner').fillna(0)\n",
    "    feat_col_name = [col for col in list(tr_feat.columns) if col not in not_col_name]\n",
    "    X = tr_feat.loc[:,feat_col_name].to_numpy()\n",
    "    return X\n",
    "\n",
    "def normalize_dat_v2(trX,ttX,axis=0):\n",
    "    center = trX.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = trX.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    trX = (trX-center)/scale\n",
    "    ttX = (ttX-center)/scale\n",
    "    return trX,ttX\n",
    "\n",
    "def get_para_normalize_dat(trX,axis=0):\n",
    "    center = trX.mean(axis=axis)\n",
    "    scale = trX.std(axis=axis)\n",
    "    scale += 1e-4\n",
    "    return center,scale\n",
    "\n",
    "def apply_para_normalize_dat(X,center,scale,axis=0):\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    X = (X-center)/scale\n",
    "    return X\n",
    "\n",
    "def normalize_dat(trX,ttX,cols=5,axis=0):\n",
    "    D = trX[:,:cols]\n",
    "    center = D.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = D.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    trX[:,:cols] = (D-center)/scale\n",
    "    ttX[:,:cols] = (ttX[:,:cols]-center)/scale\n",
    "    \n",
    "def calc_topk_acc_v2(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    \"\"\"\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    match_array = np.logical_or.reduce(max_k_cat==y_truth_cat, axis=1) #得到匹配结果\n",
    "    topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "    return topk_acc_score\n",
    "\n",
    "def calc_topk_acc_cat_all(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    return top1-topk acc\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    M = max_k_cat==y_truth_cat\n",
    "    for k in range(M.shape[1]):\n",
    "        match_array = np.logical_or.reduce(M[:,:k+1], axis=1) #得到匹配结果\n",
    "        topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "        res.append(topk_acc_score)\n",
    "    return res\n",
    "\n",
    "def transpd2np_single(featdatC,featdatL,trdat):\n",
    "    not_feat_col = ['duns_number',\n",
    "                     'atlas_location_uuid',\n",
    "                     'longitude_loc',\n",
    "                     'latitude_loc',\n",
    "                     'label']\n",
    "    cont_col_nameC = ['emp_here','emp_total','sales_volume_us','square_footage','emp_here_range']\n",
    "    cont_col_nameL = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                     'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                     'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                     'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                     'pct_masters_degree','walk_score','bike_score']\n",
    "    trXCC,trXDC,trXCL,trXDL,trY = transpd2np(featdatC,featdatL,trdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    \n",
    "    trXC = np.concatenate([trXCC,trXCL],axis=1)\n",
    "    trXD = np.concatenate([trXDC,trXDL],axis=1)\n",
    "    del trXCC,trXDC,trXCL,trXDL\n",
    "    return trXC,trXD,trY\n",
    "\n",
    "def onehot2cat(x):\n",
    "    \"\"\"\n",
    "    x: each row is a sample\n",
    "    \"\"\"\n",
    "    return [np.where(r==1)[0][0] for r in x]\n",
    "\n",
    "def get_loc_feat_by_comp(proc_comp_dat,pair_dat):\n",
    "    tr_feat = pd.merge(pair_dat[['atlas_location_uuid','duns_number']],proc_comp_dat,on='duns_number',how='inner')\n",
    "#     tr_feat = tr_feat.fillna(0)\n",
    "    tr_feat = tr_feat.groupby(['atlas_location_uuid']).mean().drop(columns=['duns_number'])\n",
    "    return tr_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/yefeichen/Database/location_recommender_system/'\n",
    "cfile = ['dnb_pa.csv','dnb_sf.csv','dnb_sj.csv']\n",
    "lfile = 'location_scorecard_190912.csv'\n",
    "clfile = ['PA.csv','SF.csv','SJ.csv']\n",
    "lfile_app = ['PA_comp_loc_score.csv']\n",
    "\n",
    "ind_city = 0\n",
    "\n",
    "pdc = pd.read_csv(pjoin(datapath,cfile[ind_city]))\n",
    "pdl = pd.read_csv(pjoin(datapath,lfile))\n",
    "pdcl = pd.read_csv(pjoin(datapath,clfile[ind_city]))\n",
    "proc_pdl_x = pd.read_csv(lfile_app[ind_city])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "6173\n",
      "Neg dat num: 12346 ;Pos dat num: 6219\n",
      "all_train_pairs 18565\n",
      "split pair into train/test\n",
      "Train dat: 14852 Test dat: 3694\n",
      "append feature with train/test pairs\n",
      "(14852, 24)\n",
      "start training...\n",
      "0.7230644288034651\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "6171\n",
      "Neg dat num: 12342 ;Pos dat num: 6219\n",
      "all_train_pairs 18561\n",
      "split pair into train/test\n",
      "Train dat: 14849 Test dat: 3697\n",
      "append feature with train/test pairs\n",
      "(14849, 24)\n",
      "start training...\n",
      "0.5279956721666216\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "6184\n",
      "Neg dat num: 12368 ;Pos dat num: 6219\n",
      "all_train_pairs 18587\n",
      "split pair into train/test\n",
      "Train dat: 14870 Test dat: 3695\n",
      "append feature with train/test pairs\n",
      "(14870, 24)\n",
      "start training...\n",
      "0.5412719891745602\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "6169\n",
      "Neg dat num: 12338 ;Pos dat num: 6219\n",
      "all_train_pairs 18557\n",
      "split pair into train/test\n",
      "Train dat: 14846 Test dat: 3696\n",
      "append feature with train/test pairs\n",
      "(14846, 24)\n",
      "start training...\n",
      "0.5262445887445888\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transfer positive pair into postive and negative pair\n",
      "6164\n",
      "Neg dat num: 12328 ;Pos dat num: 6219\n",
      "all_train_pairs 18547\n",
      "split pair into train/test\n",
      "Train dat: 14838 Test dat: 3692\n",
      "append feature with train/test pairs\n",
      "(14838, 24)\n",
      "start training...\n",
      "0.6828277356446371\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "6168\n",
      "Neg dat num: 12336 ;Pos dat num: 6219\n",
      "all_train_pairs 18555\n",
      "split pair into train/test\n",
      "Train dat: 14844 Test dat: 3698\n",
      "append feature with train/test pairs\n",
      "(14844, 24)\n",
      "start training...\n",
      "0.6592752839372634\n",
      "dummy data\n",
      "doing one-hot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "6175\n",
      "Neg dat num: 12350 ;Pos dat num: 6219\n",
      "all_train_pairs 18569\n",
      "split pair into train/test\n",
      "Train dat: 14855 Test dat: 3700\n",
      "append feature with train/test pairs\n",
      "(14855, 24)\n",
      "start training...\n",
      "0.6537837837837838\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "6165\n",
      "Neg dat num: 12330 ;Pos dat num: 6219\n",
      "all_train_pairs 18549\n",
      "split pair into train/test\n",
      "Train dat: 14839 Test dat: 3692\n",
      "append feature with train/test pairs\n",
      "(14839, 24)\n",
      "start training...\n",
      "0.7302275189599133\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "6180\n",
      "Neg dat num: 12360 ;Pos dat num: 6219\n",
      "all_train_pairs 18579\n",
      "split pair into train/test\n",
      "Train dat: 14863 Test dat: 3702\n",
      "append feature with train/test pairs\n",
      "(14863, 24)\n",
      "start training...\n",
      "0.8074014046461372\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "6160\n",
      "Neg dat num: 12320 ;Pos dat num: 6219\n",
      "all_train_pairs 18539\n",
      "split pair into train/test\n",
      "Train dat: 14831 Test dat: 3695\n",
      "append feature with train/test pairs\n",
      "(14831, 24)\n",
      "start training...\n",
      "0.5491204330175914\n",
      "Avg ROC-AUC=0.7094 +/- 0.14\n",
      "Avg PR-AUC=0.5641 +/- 0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "nfold = 10\n",
    "pr_aucs = []\n",
    "roc_aucs = []\n",
    "for n in range(nfold):\n",
    "    print('dummy data')\n",
    "    proc_pdc = comp_dat_process(pdc)\n",
    "    proc_pdl = location_dat_process(pdl)\n",
    "    # proc_pdl_x = get_loc_feat_by_comp(proc_pdc,pdcl)\n",
    "\n",
    "\n",
    "    print('transfer positive pair into postive and negative pair')\n",
    "    all_pdcl = getPosNegdat(pdcl)\n",
    "    print('all_train_pairs',len(all_pdcl))\n",
    "    print('split pair into train/test')\n",
    "    tr,tt = splitdat(all_pdcl,key_column=['duns_number','atlas_location_uuid'],right_colunm='label_tr',rate_tr=0.8)\n",
    "    print('append feature with train/test pairs')\n",
    "    trXC,trXD,ttXC,ttXD,trY,ttY = transpd2np_train_test(proc_pdc,proc_pdl['data'],tr,tt)\n",
    "    print(trXC.shape)\n",
    "    trXL_app = transpdfeat_w_pair(featdat=proc_pdl_x,pairdat=tr,key_col=['duns_number','atlas_location_uuid'],not_col_name=['duns_number','atlas_location_uuid'])\n",
    "    ttXL_app = transpdfeat_w_pair(featdat=proc_pdl_x,pairdat=tt,key_col=['duns_number','atlas_location_uuid'],not_col_name=['duns_number','atlas_location_uuid'])\n",
    "\n",
    "    trXC = np.concatenate([trXC,trXL_app],axis=1)\n",
    "    ttXC = np.concatenate([ttXC,ttXL_app],axis=1)\n",
    "\n",
    "    center,scale = get_para_normalize_dat(trXC)\n",
    "\n",
    "    trXC = apply_para_normalize_dat(trXC,center,scale)\n",
    "    ttXC = apply_para_normalize_dat(ttXC,center,scale)\n",
    "    # trXC,ttXC = normalize_dat_v2(trXC,ttXC)\n",
    "    trX = np.concatenate([trXC,trXD],axis=1)\n",
    "    ttX = np.concatenate([ttXC,ttXD],axis=1)\n",
    "\n",
    "    trX = normalize(trX,axis=1)\n",
    "    ttX = normalize(ttX,axis=1)\n",
    "\n",
    "    print('start training...')\n",
    "    clf = LR(random_state=0,fit_intercept=True,\n",
    "             solver = 'lbfgs',class_weight='balanced',\n",
    "             multi_class='ovr',max_iter=1e10).fit(trX,trY)\n",
    "    pred_trY = clf.predict(trX)\n",
    "    pred_ttY = clf.predict(ttX)\n",
    "    test_score = clf.score(ttX,ttY)\n",
    "    print(test_score)\n",
    "\n",
    "    #draw verification score\n",
    "    score_mat = clf.predict_proba(ttX)[:,1]\n",
    "    precision, recall, thresholds = precision_recall_curve(ttY, score_mat)\n",
    "\n",
    "    # Draw R/P Curve\n",
    "    def plot_pr(auc_score, precision, recall, label=None):\n",
    "        pylab.figure(num=None, figsize=(6, 5))\n",
    "        pylab.xlim([0.0, 1.0])\n",
    "        pylab.ylim([0.0, 1.0])\n",
    "        pylab.xlabel('Recall')\n",
    "        pylab.ylabel('Precision')\n",
    "        pylab.title('P/R (AUC=%0.2f) / %s' % (auc_score, label))\n",
    "        pylab.fill_between(recall, precision, alpha=0.5)\n",
    "        pylab.grid(True, linestyle='-', color='0.75')\n",
    "        pylab.plot(recall, precision, lw=1)\n",
    "        pylab.show()\n",
    "\n",
    "\n",
    "    pr_auc = auc(recall,precision)\n",
    "\n",
    "#     plot_pr(pr_auc, precision, recall, \"pos\")\n",
    "\n",
    "    fpr, tpr, roc_thresholds = roc_curve(ttY, score_mat)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "#     x = list(fpr)\n",
    "#     y = list(tpr)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(x,y)\n",
    "#     plt.xlabel(\"fpr\")\n",
    "#     plt.ylabel(\"tpr\")\n",
    "#     plt.title(\"ROC-curve(AUC=%0.2f) of %s\"%(roc_auc,cfile[ind_city]))\n",
    "#     if roc_auc > 0.9:\n",
    "#         break\n",
    "    \n",
    "    pr_aucs.append(pr_auc)\n",
    "    roc_aucs.append(roc_auc)\n",
    "\n",
    "avg_pr_auc = np.array(pr_aucs).mean()\n",
    "std_pr_auc = np.array(pr_aucs).std()\n",
    "avg_roc_auc = np.array(roc_aucs).mean()\n",
    "std_roc_auc = np.array(roc_aucs).std()\n",
    "print('Avg ROC-AUC=%0.4f +/- %0.2f'% (avg_roc_auc,std_roc_auc))\n",
    "print('Avg PR-AUC=%0.4f +/- %0.2f' % (avg_pr_auc,std_pr_auc))\n",
    "# del ttX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3692,), 1577, (14830,), 6300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ttY.shape,pred_ttY.sum(),pred_trY.shape,pred_trY.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mat = clf.predict_proba(trX)[:,1]\n",
    "fpr, tpr, roc_thresholds = roc_curve(trY, score_mat)\n",
    "roc_auc = auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5612885832640891"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duns_number</th>\n",
       "      <th>atlas_location_uuid</th>\n",
       "      <th>sim_score_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2035902.0</td>\n",
       "      <td>003fd16c-0dee-24a1-1adf-0b47d6120f3d</td>\n",
       "      <td>4542.242728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2035902.0</td>\n",
       "      <td>0060ef47-b121-60b9-1983-02ebdb144dcf</td>\n",
       "      <td>13094.133076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2035902.0</td>\n",
       "      <td>0084ae4b-c24c-0795-d1e6-a4f58444d39b</td>\n",
       "      <td>9791.059136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2035902.0</td>\n",
       "      <td>00a68dc6-f955-c581-9a54-64404bb0ef6a</td>\n",
       "      <td>15262.271423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2035902.0</td>\n",
       "      <td>01109439-6b76-0fc9-2d96-cfdb79708a80</td>\n",
       "      <td>3614.159930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duns_number                   atlas_location_uuid  sim_score_new\n",
       "0    2035902.0  003fd16c-0dee-24a1-1adf-0b47d6120f3d    4542.242728\n",
       "1    2035902.0  0060ef47-b121-60b9-1983-02ebdb144dcf   13094.133076\n",
       "2    2035902.0  0084ae4b-c24c-0795-d1e6-a4f58444d39b    9791.059136\n",
       "3    2035902.0  00a68dc6-f955-c581-9a54-64404bb0ef6a   15262.271423\n",
       "4    2035902.0  01109439-6b76-0fc9-2d96-cfdb79708a80    3614.159930"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_pdl_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
