{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe each location with companies in side\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pygeohash as pgh\n",
    "from math import *\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from matplotlib import pylab\n",
    "from sklearn.preprocessing import normalize\n",
    "pjoin = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function_base\n",
    "def getPosNegdat(dat):\n",
    "    \"\"\"\n",
    "    dat: pos pair of data (location,company,geo,distance)\n",
    "    return pos/neg pair of data, same structure of dat except one more column for label\n",
    "    \"\"\"\n",
    "    shuffle_dat = dat.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # shuffle_dat.head()\n",
    "\n",
    "    twin_dat = dat.join(shuffle_dat,how='left',lsuffix='_left',rsuffix='_right')\n",
    "    twin_dat = twin_dat[twin_dat['atlas_location_uuid_left'] != twin_dat['atlas_location_uuid_right']]\n",
    "    print(len(twin_dat))\n",
    "    twin_dat.head()\n",
    "\n",
    "    neg_datA = twin_dat[['duns_number_left','atlas_location_uuid_right','longitude_loc_right','latitude_loc_right']]\n",
    "    neg_datA = neg_datA.rename(columns={'duns_number_left':'duns_number','atlas_location_uuid_right':'atlas_location_uuid','longitude_loc_right':'longitude_loc','latitude_loc_right':'latitude_loc'})\n",
    "\n",
    "    neg_datB = twin_dat[['duns_number_right','atlas_location_uuid_left','longitude_loc_left','latitude_loc_left']]\n",
    "    neg_datB = neg_datB.rename(columns={'duns_number_right':'duns_number','atlas_location_uuid_left':'atlas_location_uuid','longitude_loc_left':'longitude_loc','latitude_loc_left':'latitude_loc'})\n",
    "\n",
    "    neg_dat = pd.concat([neg_datA,neg_datB],axis=0)\n",
    "    neg_dat['label'] = 0\n",
    "    dat['label'] = 1\n",
    "    res_dat = pd.concat([dat[['duns_number','atlas_location_uuid','longitude_loc','latitude_loc','label']],neg_dat],axis=0)\n",
    "    print('Neg dat num:',len(neg_dat),';Pos dat num:',len(dat))\n",
    "    return res_dat\n",
    "\n",
    "def splitdat(dat,key_column=['duns_number'],right_colunm='atlas_location_uuid_tr',rate_tr=0.8):\n",
    "    \"\"\"\n",
    "    split the <company,location> pair into training/testing dat\n",
    "    \"\"\"\n",
    "    tr = dat.sample(frac=rate_tr)\n",
    "    tt = pd.merge(dat,tr,on=key_column,how='left',suffixes=['','_tr'])\n",
    "    tt = tt[tt[right_colunm].isnull()]\n",
    "    tt = tt[list(tr.columns)]\n",
    "    print('Train dat:', len(tr), 'Test dat:', len(tt))\n",
    "    return tr,tt\n",
    "\n",
    "#data process\n",
    "def onehotdat(dat,key_column:list,dummy_na=True):\n",
    "    dat[key_column] = dat[key_column].astype(str)\n",
    "    dum_dat = pd.get_dummies(dat[key_column],dummy_na=dummy_na)#it has nan itself\n",
    "    return dum_dat\n",
    "\n",
    "def split2num(emp_range:str):\n",
    "    max_emp_val = emp_range.replace(' ','').split('-')\n",
    "    if len(max_emp_val)<2:\n",
    "        return 10\n",
    "    else:\n",
    "        return float(max_emp_val[1])\n",
    "    \n",
    "def max_col(dat,col,minval=1):\n",
    "    dat[col] = dat[col].apply(lambda r:max(r,minval))\n",
    "\n",
    "def comp_dat_process(dat):\n",
    "    \"\"\"\n",
    "    pd -> company key,cont_feature,spec_feature,dum_feature\n",
    "    \"\"\"\n",
    "    one_hot_col_name = ['major_industry_category','location_type','primary_sic_2_digit']\n",
    "    spec_col_name = 'emp_here_range'\n",
    "    cont_col_name = ['emp_here','emp_total','sales_volume_us','square_footage']\n",
    "\n",
    "    print('doing one-hot...')\n",
    "    dum_dat = onehotdat(dat,one_hot_col_name)\n",
    "    \n",
    "    print('extract continuous...')\n",
    "    cont_dat = dat[cont_col_name].fillna(value=0).astype(float)\n",
    "    \n",
    "    print('specific feature')\n",
    "    spec_dat = dat[spec_col_name].fillna(value='1-10').astype(str)\n",
    "    spec_dat = spec_dat.apply(lambda row: split2num(row))\n",
    "    \n",
    "    max_col(cont_dat,'emp_here',1)\n",
    "    \n",
    "    res_dat = dat[['duns_number']].join([cont_dat,spec_dat,dum_dat],how='left')\n",
    "    assert(len(res_dat)==len(dum_dat))\n",
    "    assert(len(res_dat)==len(cont_dat))\n",
    "    assert(len(res_dat)==len(spec_dat))\n",
    "    return res_dat\n",
    "\n",
    "def location_dat_process(dat):\n",
    "    \"\"\"\n",
    "    pd -> location key,cont_feature,dum_feature\n",
    "    \"\"\"\n",
    "    one_hot_col_name = ['building_class']\n",
    "    cont_col_name = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                     'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                     'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                     'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                     'pct_masters_degree','walk_score','bike_score']\n",
    "\n",
    "    print('doing one-hot...')\n",
    "    dum_dat = onehotdat(dat,one_hot_col_name,False)\n",
    "    \n",
    "    print('extract continuous...')\n",
    "    cont_dat = dat[cont_col_name].fillna(value=0).astype(float)\n",
    "    \n",
    "    res_dat = dat[['atlas_location_uuid']].join([cont_dat,dum_dat],how='left')\n",
    "    assert(len(res_dat)==len(dum_dat))\n",
    "    assert(len(res_dat)==len(cont_dat))\n",
    "    return {'data':res_dat,\n",
    "            'cont_feat_num':len(list(cont_dat.columns)),\n",
    "            'dum_feat_num':len(list(dum_dat.columns))}\n",
    "\n",
    "def comp_transpd2np(featdat,trdat,ttdat,not_col_name):\n",
    "    tr_feat = pd.merge(trdat,featdat,on='duns_number',how='inner')\n",
    "#     print(col_list)\n",
    "    col_list = [ n for n in list(tr_feat.columns) if n not in not_col_name ] \n",
    "    trainX = tr_feat.loc[:,col_list].to_numpy()\n",
    "    trainY = tr_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    \n",
    "    tt_feat = pd.merge(ttdat,featdat,on='duns_number',how='inner')\n",
    "    col_list = [ n for n in list(tt_feat.columns) if n not in not_col_name ] \n",
    "#     print(col_list)\n",
    "    testX = tt_feat.loc[:,col_list].to_numpy()\n",
    "    testY = tt_feat[['atlas_location_uuid','longitude_loc','latitude_loc']].to_numpy()\n",
    "    return trainX,trainY,testX,testY\n",
    "\n",
    "def transpd2np(featdatC,featdatL,pairdat,cont_col_nameC,cont_col_nameL,not_feat_col):\n",
    "    tr_feat = pd.merge(pairdat,featdatC,on='duns_number',how='inner')\n",
    "    XCC = tr_feat.loc[:,cont_col_nameC].to_numpy()\n",
    "    out_col = []\n",
    "    out_col.extend(not_feat_col)\n",
    "    out_col.extend(cont_col_nameC)\n",
    "    dum_col_nameC = [col for col in list(tr_feat.columns) if col not in out_col]\n",
    "    XDC = tr_feat.loc[:,dum_col_nameC].to_numpy()\n",
    "\n",
    "    tr_feat = pd.merge(pairdat,featdatL,on='atlas_location_uuid',how='inner')\n",
    "    XCL = tr_feat.loc[:,cont_col_nameL].to_numpy()\n",
    "    out_col = []\n",
    "    out_col.extend(not_feat_col)\n",
    "    out_col.extend(cont_col_nameL)\n",
    "    dum_col_nameL = [col for col in list(tr_feat.columns) if col not in out_col]\n",
    "    XDL = tr_feat.loc[:,dum_col_nameL].to_numpy()\n",
    "\n",
    "    Y = pairdat[['label']].to_numpy()\n",
    "    return XCC,XDC,XCL,XDL,Y\n",
    "\n",
    "def transpd2np_train_test(featdatC,featdatL,trdat,ttdat):\n",
    "    not_feat_col = ['duns_number',\n",
    "                     'atlas_location_uuid',\n",
    "                     'longitude_loc',\n",
    "                     'latitude_loc',\n",
    "                     'label']\n",
    "    cont_col_nameC = ['emp_here','emp_total','sales_volume_us','square_footage','emp_here_range']\n",
    "    cont_col_nameL = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                     'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                     'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                     'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                     'pct_masters_degree','walk_score','bike_score']\n",
    "    trXCC,trXDC,trXCL,trXDL,trY = transpd2np(featdatC,featdatL,trdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    ttXCC,ttXDC,ttXCL,ttXDL,ttY = transpd2np(featdatC,featdatL,ttdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    \n",
    "    trXC = np.concatenate([trXCC,trXCL],axis=1)\n",
    "    trXD = np.concatenate([trXDC,trXDL],axis=1)\n",
    "    ttXC = np.concatenate([ttXCC,ttXCL],axis=1)\n",
    "    ttXD = np.concatenate([ttXDC,ttXDL],axis=1)\n",
    "#     trXC = 1.0*trXCC\n",
    "#     trXD = 1.0*trXDC\n",
    "#     ttXC = 1.0*ttXCC\n",
    "#     ttXD = 1.0*ttXDC\n",
    "    del trXCC,trXDC,trXCL,trXDL,ttXCC,ttXDC,ttXCL,ttXDL\n",
    "    return trXC,trXD,ttXC,ttXD,trY,ttY\n",
    "\n",
    "def transpd2np_train_test_detail(featdatC,featdatL,trdat,ttdat):\n",
    "    not_feat_col = ['duns_number',\n",
    "                     'atlas_location_uuid',\n",
    "                     'longitude_loc',\n",
    "                     'latitude_loc',\n",
    "                     'label']\n",
    "    cont_col_nameC = ['emp_here','emp_total','sales_volume_us','square_footage','emp_here_range']\n",
    "    cont_col_nameL = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                     'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                     'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                     'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                     'pct_masters_degree','walk_score','bike_score']\n",
    "    trXCC,trXDC,trXCL,trXDL,trY = transpd2np(featdatC,featdatL,trdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    ttXCC,ttXDC,ttXCL,ttXDL,ttY = transpd2np(featdatC,featdatL,ttdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    \n",
    "#     trXC = np.concatenate([trXCC,trXCL],axis=1)\n",
    "#     trXD = np.concatenate([trXDC,trXDL],axis=1)\n",
    "#     ttXC = np.concatenate([ttXCC,ttXCL],axis=1)\n",
    "#     ttXD = np.concatenate([ttXDC,ttXDL],axis=1)\n",
    "#     trXC = 1.0*trXCC\n",
    "#     trXD = 1.0*trXDC\n",
    "#     ttXC = 1.0*ttXCC\n",
    "#     ttXD = 1.0*ttXDC\n",
    "    return trXCC,trXDC,trXCL,trXDL,ttXCC,ttXDC,ttXCL,ttXDL,trY,ttY\n",
    "\n",
    "def transpdfeat_w_pair(featdat,pairdat,key_col,not_col_name):\n",
    "    tr_feat = pd.merge(pairdat,featdat,on=key_col,how='inner').fillna(0)\n",
    "    feat_col_name = [col for col in list(tr_feat.columns) if col not in not_col_name]\n",
    "    X = tr_feat.loc[:,feat_col_name].to_numpy()\n",
    "    return X\n",
    "\n",
    "def normalize_dat_v2(trX,ttX,axis=0):\n",
    "    center = trX.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = trX.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    trX = (trX-center)/scale\n",
    "    ttX = (ttX-center)/scale\n",
    "    return trX,ttX\n",
    "\n",
    "def get_para_normalize_dat(trX,axis=0):\n",
    "    center = trX.mean(axis=axis)\n",
    "    scale = trX.std(axis=axis)\n",
    "    scale += 1e-4\n",
    "    return center,scale\n",
    "\n",
    "def apply_para_normalize_dat(X,center,scale,axis=0):\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    X = (X-center)/scale\n",
    "    return X\n",
    "\n",
    "def normalize_dat(trX,ttX,cols=5,axis=0):\n",
    "    D = trX[:,:cols]\n",
    "    center = D.mean(axis=axis)\n",
    "    center = np.expand_dims(center,axis)\n",
    "    scale = D.std(axis=axis)\n",
    "    scale = np.expand_dims(scale,axis)\n",
    "    \n",
    "    trX[:,:cols] = (D-center)/scale\n",
    "    ttX[:,:cols] = (ttX[:,:cols]-center)/scale\n",
    "    \n",
    "def calc_topk_acc_v2(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    \"\"\"\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    match_array = np.logical_or.reduce(max_k_cat==y_truth_cat, axis=1) #得到匹配结果\n",
    "    topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "    return topk_acc_score\n",
    "\n",
    "def calc_topk_acc_cat_all(QRscore,y_truth_cat,R_cat,k=3):\n",
    "    \"\"\"\n",
    "    QRscore: similarity score matrix shape [Q,R]\n",
    "    y_truth: index(related with R) of truth label of Query\n",
    "    return top1-topk acc\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    y_truth_cat = y_truth_cat.reshape(-1,1)\n",
    "    max_k_preds = QRscore.argsort(axis=1)[:, -k:][:, ::-1] #得到top-k max label\n",
    "    max_k_cat = R_cat[max_k_preds]\n",
    "    M = max_k_cat==y_truth_cat\n",
    "    for k in range(M.shape[1]):\n",
    "        match_array = np.logical_or.reduce(M[:,:k+1], axis=1) #得到匹配结果\n",
    "        topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "        res.append(topk_acc_score)\n",
    "    return res\n",
    "\n",
    "def transpd2np_single(featdatC,featdatL,trdat):\n",
    "    not_feat_col = ['duns_number',\n",
    "                     'atlas_location_uuid',\n",
    "                     'longitude_loc',\n",
    "                     'latitude_loc',\n",
    "                     'label']\n",
    "    cont_col_nameC = ['emp_here','emp_total','sales_volume_us','square_footage','emp_here_range']\n",
    "    cont_col_nameL = ['score_predicted_eo','score_employer','num_emp_weworkcore','num_poi_weworkcore',\n",
    "                     'pct_wwcore_employee','pct_wwcore_business','num_retail_stores','num_doctor_offices',\n",
    "                     'num_eating_places','num_drinking_places','num_hotels','num_fitness_gyms',\n",
    "                     'population_density','pct_female_population','median_age','income_per_capita',\n",
    "                     'pct_masters_degree','walk_score','bike_score']\n",
    "    trXCC,trXDC,trXCL,trXDL,trY = transpd2np(featdatC,featdatL,trdat,cont_col_nameC,cont_col_nameL,not_feat_col)\n",
    "    \n",
    "    trXC = np.concatenate([trXCC,trXCL],axis=1)\n",
    "    trXD = np.concatenate([trXDC,trXDL],axis=1)\n",
    "    del trXCC,trXDC,trXCL,trXDL\n",
    "    return trXC,trXD,trY\n",
    "\n",
    "def onehot2cat(x):\n",
    "    \"\"\"\n",
    "    x: each row is a sample\n",
    "    \"\"\"\n",
    "    return [np.where(r==1)[0][0] for r in x]\n",
    "\n",
    "def get_loc_feat_by_comp(proc_comp_dat,pair_dat):\n",
    "    tr_feat = pd.merge(pair_dat[['atlas_location_uuid','duns_number']],proc_comp_dat,on='duns_number',how='inner')\n",
    "#     tr_feat = tr_feat.fillna(0)\n",
    "    tr_feat = tr_feat.groupby(['atlas_location_uuid']).mean().drop(columns=['duns_number'])\n",
    "    return tr_feat\n",
    "\n",
    "def featCross(Xa,Xb):\n",
    "    \"\"\"\n",
    "    Xa [Sample,featdimA]\n",
    "    Xb [Sample,featdimB]\n",
    "    return [Sample,featdimA*featdimB]\n",
    "    \"\"\"\n",
    "    na,da = Xa.shape\n",
    "    nb,db = Xb.shape\n",
    "    assert(na==nb)\n",
    "    Xab = []\n",
    "    for i in range(da):\n",
    "        Xaib = Xa[:,i].reshape(-1,1)*Xb\n",
    "        Xab.append(Xaib)\n",
    "    Xab = np.concatenate(Xab,axis=1)\n",
    "    return Xab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/yefeichen/Database/location_recommender_system/'\n",
    "cfile = ['dnb_pa.csv','dnb_sf.csv','dnb_sj.csv']\n",
    "lfile = 'location_scorecard_190912.csv'\n",
    "clfile = ['PA.csv','SF.csv','SJ.csv']\n",
    "lfile_app = ['PA_comp_loc_score.csv','SF_comp_loc_score.csv','SJ_comp_loc_score.csv']\n",
    "\n",
    "ind_city = 2\n",
    "\n",
    "pdc = pd.read_csv(pjoin(datapath,cfile[ind_city]))\n",
    "pdl = pd.read_csv(pjoin(datapath,lfile))\n",
    "pdcl = pd.read_csv(pjoin(datapath,clfile[ind_city]))\n",
    "# proc_pdl_x = pd.read_csv(lfile_app[ind_city])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "24953\n",
      "Neg dat num: 49906 ;Pos dat num: 25040\n",
      "all_train_pairs 74946\n",
      "split pair into train/test\n",
      "Train dat: 59957 Test dat: 14965\n",
      "append feature with train/test pairs\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4988974273304377\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "24967\n",
      "Neg dat num: 49934 ;Pos dat num: 25040\n",
      "all_train_pairs 74974\n",
      "split pair into train/test\n",
      "Train dat: 59979 Test dat: 14963\n",
      "append feature with train/test pairs\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5008353939717971\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "24952\n",
      "Neg dat num: 49904 ;Pos dat num: 25040\n",
      "all_train_pairs 74944\n",
      "split pair into train/test\n",
      "Train dat: 59955 Test dat: 14952\n",
      "append feature with train/test pairs\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49130551096843234\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "24962\n",
      "Neg dat num: 49924 ;Pos dat num: 25040\n",
      "all_train_pairs 74964\n",
      "split pair into train/test\n",
      "Train dat: 59971 Test dat: 14968\n",
      "append feature with train/test pairs\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5073490112239444\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "24940\n",
      "Neg dat num: 49880 ;Pos dat num: 25040\n",
      "all_train_pairs 74920\n",
      "split pair into train/test\n",
      "Train dat: 59936 Test dat: 14962\n",
      "append feature with train/test pairs\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48609811522523727\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "24944\n",
      "Neg dat num: 49888 ;Pos dat num: 25040\n",
      "all_train_pairs 74928\n",
      "split pair into train/test\n",
      "Train dat: 59942 Test dat: 14962\n",
      "append feature with train/test pairs\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.509089693891191\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "24950\n",
      "Neg dat num: 49900 ;Pos dat num: 25040\n",
      "all_train_pairs 74940\n",
      "split pair into train/test\n",
      "Train dat: 59952 Test dat: 14964\n",
      "append feature with train/test pairs\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49438652766639934\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "24947\n",
      "Neg dat num: 49894 ;Pos dat num: 25040\n",
      "all_train_pairs 74934\n",
      "split pair into train/test\n",
      "Train dat: 59947 Test dat: 14966\n",
      "append feature with train/test pairs\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5378858746492049\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "24963\n",
      "Neg dat num: 49926 ;Pos dat num: 25040\n",
      "all_train_pairs 74966\n",
      "split pair into train/test\n",
      "Train dat: 59973 Test dat: 14966\n",
      "append feature with train/test pairs\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5074836295603368\n",
      "dummy data\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "specific feature\n",
      "doing one-hot...\n",
      "extract continuous...\n",
      "transfer positive pair into postive and negative pair\n",
      "24945\n",
      "Neg dat num: 49890 ;Pos dat num: 25040\n",
      "all_train_pairs 74930\n",
      "split pair into train/test\n",
      "Train dat: 59944 Test dat: 14967\n",
      "append feature with train/test pairs\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefeichen/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5127948152602392\n",
      "Avg ROC-AUC=0.5012 +/- 0.02\n",
      "Avg PR-AUC=0.3361 +/- 0.01\n"
     ]
    }
   ],
   "source": [
    "nfold = 10\n",
    "pr_aucs = []\n",
    "roc_aucs = []\n",
    "for n in range(nfold):\n",
    "    print('dummy data')\n",
    "    proc_pdc = comp_dat_process(pdc)\n",
    "    proc_pdl = location_dat_process(pdl)\n",
    "#     proc_pdl_x = get_loc_feat_by_comp(proc_pdc,pdcl)\n",
    "\n",
    "\n",
    "    print('transfer positive pair into postive and negative pair')\n",
    "    all_pdcl = getPosNegdat(pdcl)\n",
    "    print('all_train_pairs',len(all_pdcl))\n",
    "    print('split pair into train/test')\n",
    "    tr,tt = splitdat(all_pdcl,key_column=['duns_number','atlas_location_uuid'],right_colunm='label_tr',rate_tr=0.8)\n",
    "    print('append feature with train/test pairs')\n",
    "    trXCC,trXDC,trXCL,trXDL,ttXCC,ttXDC,ttXCL,ttXDL,trY,ttY = transpd2np_train_test_detail(proc_pdc,proc_pdl['data'],tr,tt)\n",
    "#     print(trXC.shape)\n",
    "    \n",
    "#     trXL_app = transpdfeat_w_pair(featdat=proc_pdl_x,pairdat=tr,key_col=['duns_number','atlas_location_uuid'],not_col_name=['duns_number','atlas_location_uuid'])\n",
    "#     ttXL_app = transpdfeat_w_pair(featdat=proc_pdl_x,pairdat=tt,key_col=['duns_number','atlas_location_uuid'],not_col_name=['duns_number','atlas_location_uuid'])\n",
    "#     trXC = np.concatenate([trXC,trXL_app],axis=1)\n",
    "#     ttXC = np.concatenate([ttXC,ttXL_app],axis=1)\n",
    "\n",
    "    centerC,scaleC = get_para_normalize_dat(trXCC)\n",
    "    centerL,scaleL = get_para_normalize_dat(trXCL)\n",
    "\n",
    "    trXCC = apply_para_normalize_dat(trXCC,centerC,scaleC)\n",
    "    trXCL = apply_para_normalize_dat(trXCL,centerL,scaleL)\n",
    "    ttXCC = apply_para_normalize_dat(ttXCC,centerC,scaleC)\n",
    "    ttXCL = apply_para_normalize_dat(ttXCL,centerL,scaleL)\n",
    "    # trXC,ttXC = normalize_dat_v2(trXC,ttXC)\n",
    "    \n",
    "    trXC = np.concatenate([trXCC,trXDC],axis=1)\n",
    "    trXL = np.concatenate([trXCL,trXDL],axis=1)\n",
    "    ttXC = np.concatenate([ttXCC,ttXDC],axis=1)\n",
    "    ttXL = np.concatenate([ttXCL,ttXDL],axis=1)\n",
    "    del trXCC,trXDC,trXCL,trXDL,ttXCC,ttXDC,ttXCL,ttXDL\n",
    "    \n",
    "    trXCxL = featCross(trXC,trXL)\n",
    "    ttXCxL = featCross(ttXC,ttXL)\n",
    "    \n",
    "    trX = np.concatenate([trXC,trXL,trXCxL],axis=1)\n",
    "    ttX = np.concatenate([ttXC,ttXL,ttXCxL],axis=1)\n",
    "    \n",
    "    del trXC,trXL,trXCxL,ttXC,ttXL,ttXCxL\n",
    "\n",
    "    trX = normalize(trX,axis=1)\n",
    "    ttX = normalize(ttX,axis=1)\n",
    "\n",
    "    print('start training...')\n",
    "    clf = LR(random_state=0,fit_intercept=True,\n",
    "             solver = 'lbfgs',class_weight='balanced',\n",
    "             multi_class='ovr',max_iter=1e10).fit(trX,trY)\n",
    "    pred_trY = clf.predict(trX)\n",
    "    pred_ttY = clf.predict(ttX)\n",
    "    test_score = clf.score(ttX,ttY)\n",
    "    print(test_score)\n",
    "\n",
    "    #draw verification score\n",
    "    score_mat = clf.predict_proba(ttX)[:,1]\n",
    "    precision, recall, thresholds = precision_recall_curve(ttY, score_mat)\n",
    "\n",
    "    # Draw R/P Curve\n",
    "    def plot_pr(auc_score, precision, recall, label=None):\n",
    "        pylab.figure(num=None, figsize=(6, 5))\n",
    "        pylab.xlim([0.0, 1.0])\n",
    "        pylab.ylim([0.0, 1.0])\n",
    "        pylab.xlabel('Recall')\n",
    "        pylab.ylabel('Precision')\n",
    "        pylab.title('P/R (AUC=%0.2f) / %s' % (auc_score, label))\n",
    "        pylab.fill_between(recall, precision, alpha=0.5)\n",
    "        pylab.grid(True, linestyle='-', color='0.75')\n",
    "        pylab.plot(recall, precision, lw=1)\n",
    "        pylab.show()\n",
    "\n",
    "\n",
    "    pr_auc = auc(recall,precision)\n",
    "\n",
    "    fpr, tpr, roc_thresholds = roc_curve(ttY, score_mat)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    \n",
    "    pr_aucs.append(pr_auc)\n",
    "    roc_aucs.append(roc_auc)\n",
    "\n",
    "avg_pr_auc = np.array(pr_aucs).mean()\n",
    "std_pr_auc = np.array(pr_aucs).std()\n",
    "avg_roc_auc = np.array(roc_aucs).mean()\n",
    "std_roc_auc = np.array(roc_aucs).std()\n",
    "print('Avg ROC-AUC=%0.4f +/- %0.2f'% (avg_roc_auc,std_roc_auc))\n",
    "print('Avg PR-AUC=%0.4f +/- %0.2f' % (avg_pr_auc,std_pr_auc))\n",
    "# del ttX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa = np.random.random((3,4))\n",
    "Xb = np.random.random((3,5))\n",
    "\n",
    "Xab = featCross(Xa,Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14839, 2231)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3691, 2231)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
